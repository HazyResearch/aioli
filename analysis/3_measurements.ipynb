{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse \n",
    "import pandas as pd \n",
    "import os \n",
    "import numpy as np\n",
    "import torch \n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from law import ScalingLaw\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import pickle \n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $sim(\\tilde{A}^t, A^{t \\star})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_weights(weight_str):\n",
    "    if len(weight_str) == 2:\n",
    "        a = int(weight_str[0])\n",
    "        b = int(weight_str[1])\n",
    "    elif len(weight_str) == 3:\n",
    "        if weight_str[0] == \"0\":\n",
    "            a = 0\n",
    "            b = int(weight_str[1:])\n",
    "        elif weight_str[-1] == \"0\":\n",
    "            b = 0 \n",
    "            a = int(weight_str[:2])\n",
    "    else:\n",
    "        idx = [i for i, ltr in enumerate(weight_str) if ltr == \".\"][1] - 1\n",
    "        a = float(weight_str[:idx])\n",
    "        b = float(weight_str[idx:])\n",
    "    return a, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params(slice_list, file_name):\n",
    "    slice_str = \"_\".join(slice_list)\n",
    "    params_file = f\"./law_results/{slice_str}/{file_name}\"\n",
    "\n",
    "    with open(params_file, 'rb') as f:\n",
    "        params = pickle.load(f)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_A_matrix_dynamic_3(params, slice_list, step, n_seeds):\n",
    "    A = np.zeros((n_seeds, 3, 3))\n",
    "\n",
    "    for i in range(n_seeds):\n",
    "\n",
    "        l1 = params[slice_list[0]][step][i]\n",
    "        l2 = params[slice_list[1]][step][i]\n",
    "        l3 = params[slice_list[2]][step][i]\n",
    "\n",
    "        A[i] = np.array([l1, l2, l3])\n",
    "\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_break_losses(dirs, slice_list, task_name, filter_word=None):\n",
    "    files_to_exclude = ['trace', 'val_inputs', 'labels', 'proportions', 'emb', 'rouge', 'generations', 'gradient', 'acc', 'matrices']    \n",
    "    df_all = pd.DataFrame() \n",
    "\n",
    "    for dir in dirs:\n",
    "        files = os.listdir(dir)\n",
    "        files = [os.path.join(dir, f) for f in files]\n",
    "        files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "\n",
    "        for file in files:\n",
    "            if \".log\" in file or task_name not in file:\n",
    "                continue \n",
    "\n",
    "            if any(skill not in file for skill in slice_list):\n",
    "                continue \n",
    "\n",
    "            if \"break\" not in file:\n",
    "                continue\n",
    "\n",
    "            if filter_word is not None:\n",
    "                if filter_word not in file:\n",
    "                    continue\n",
    "\n",
    "            break_steps = int(file.split(\"break_\")[-1].split(\"_\")[0])\n",
    "           \n",
    "            method = file.split(\"/\")[-1]\n",
    "\n",
    "\n",
    "            runs = os.listdir(file)\n",
    "            for run in runs:\n",
    "\n",
    "                if \"test_\" in run:\n",
    "                    continue\n",
    "\n",
    "                if any([exclude_file in run for exclude_file in files_to_exclude]):\n",
    "                    continue \n",
    "\n",
    "                seed = int(run.split(\"seed_\")[-1].split(\"_\")[0])\n",
    "                checkpoint = int(run.split(\"-\")[-1].split(\".\")[0])\n",
    "\n",
    "\n",
    "                path = os.path.join(file, run)\n",
    "\n",
    "                df = pd.read_pickle(path)\n",
    "\n",
    "                df = df.rename(columns={\"task_idx\": \"skill\", \"task_loss\": \"loss\"})\n",
    "\n",
    "                df[\"method\"] = method\n",
    "                df[\"seed\"] = seed\n",
    "                df[\"checkpoint\"] = checkpoint\n",
    "                df[\"break_steps\"] = break_steps\n",
    "\n",
    "\n",
    "                df.set_index(\"checkpoint\", inplace=True)\n",
    "\n",
    "\n",
    "                df_all = pd.concat([df_all, df])\n",
    "\n",
    "\n",
    "    df_all = df_all.sort_values(by=[\"checkpoint\", \"seed\"])\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doge_matrices(dirs, slice_list, break_steps):\n",
    "    matrices = {}\n",
    "\n",
    "    slice_str = \"_\".join(slice_list)\n",
    "    for dir in dirs:\n",
    "        files = os.listdir(dir)\n",
    "        files = [os.path.join(dir, f) for f in files]\n",
    "\n",
    "        for file in files:\n",
    "            \n",
    "            if \"break\" in file and f\"stratified_{slice_str}_doge_trainer\" in file:\n",
    "                s = int(file.split(\"_break_\")[-1].split(\"_\")[0])\n",
    "                if s != break_steps:\n",
    "                    continue\n",
    "                runs = os.listdir(file)\n",
    "                for run in runs:\n",
    "                    if \"avg\" not in run:\n",
    "                        continue\n",
    "                    if \"doge_matrices.npy\" not in run:\n",
    "                        continue \n",
    "                    path = os.path.join(file, run)\n",
    "                    print(path)\n",
    "                    seed = int(run.split(\"seed_\")[-1].split(\"_\")[0])\n",
    "\n",
    "                    \n",
    "\n",
    "                    A = np.load(path)\n",
    "                    matrices[seed] = A.mean(axis=0)\n",
    "\n",
    "    return matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doremi_matrices(dirs, slice_list, break_steps):\n",
    "    matrices = {}\n",
    "\n",
    "    slice_str = \"_\".join(slice_list)\n",
    "    for dir in dirs:\n",
    "        files = os.listdir(dir)\n",
    "        files = [os.path.join(dir, f) for f in files]\n",
    "\n",
    "        for file in files:\n",
    "            \n",
    "            if \"break\" in file and f\"stratified_{slice_str}_doremi\" in file:\n",
    "                s = int(file.split(\"_break_\")[-1].split(\"_\")[0])\n",
    "                if s != break_steps:\n",
    "                    continue\n",
    "                runs = os.listdir(file)\n",
    "                for run in runs:\n",
    "                    if \"avg\" not in run:\n",
    "                        continue\n",
    "                    if \"drm_matrices.npy\" not in run:\n",
    "                        continue \n",
    "                    path = os.path.join(file, run)\n",
    "                    print(path)\n",
    "                    seed = int(run.split(\"seed_\")[-1].split(\"_\")[0])\n",
    "\n",
    "                    \n",
    "\n",
    "                    A = np.load(path)\n",
    "                    matrices[seed] = A.mean(axis=0)\n",
    "\n",
    "    return matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aioli_matrices(dirs, slice_list, break_steps):\n",
    "    matrices = {}\n",
    "\n",
    "    slice_str = \"_\".join(slice_list)\n",
    "    for dir in dirs:\n",
    "        files = os.listdir(dir)\n",
    "        files = [os.path.join(dir, f) for f in files]\n",
    "\n",
    "        for file in files:\n",
    "            \n",
    "            if \"break\" in file and f\"aioli\" in file and slice_str in file:\n",
    "                s = int(file.split(\"_break_\")[-1].split(\"_\")[0])\n",
    "                if s != break_steps:\n",
    "                    continue\n",
    "                runs = os.listdir(file)\n",
    "                for run in runs:\n",
    "                    if \"aioli_matrices.npy\" not in run:\n",
    "                        continue \n",
    "                    path = os.path.join(file, run)\n",
    "                    print(path)\n",
    "                    seed = int(run.split(\"seed_\")[-1].split(\"_\")[0])\n",
    "\n",
    "                    \n",
    "\n",
    "                    A = np.load(path)\n",
    "                    matrices[seed] = A[-4:].mean(axis=0)\n",
    "\n",
    "    return matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_A_matrix_doge_3(slice_list, params_doge, steps, dirs):\n",
    "    A_doge = load_doge_matrices(dirs, slice_list, steps)\n",
    "    A_doge = np.array(list(dict(sorted(A_doge.items())).values()))\n",
    "    \n",
    "    b_t = np.array([p[0] for _, p in params_doge[steps].items()])\n",
    "\n",
    "    A_doge = A_doge * b_t[:, np.newaxis, np.newaxis]\n",
    "\n",
    "    return A_doge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_A_matrix_drm_3(slice_list, params_doremi, steps, dirs):\n",
    "    A_doremi = load_doremi_matrices(dirs, slice_list, steps)\n",
    "    A_doremi = np.array(list(dict(sorted(A_doremi.items())).values()))\n",
    "    \n",
    "    b_t = np.array([p[0] for _, p in params_doremi[steps].items()])\n",
    "\n",
    "    A_doremi = A_doremi * b_t[:, np.newaxis, np.newaxis]\n",
    "\n",
    "    return A_doremi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_A_matrix_aioli_3(slice_list, params_doremi, steps, dirs):\n",
    "    A_doremi = load_aioli_matrices(dirs, slice_list, steps)\n",
    "    A_doremi = np.array(list(dict(sorted(A_doremi.items())).values()))\n",
    "    \n",
    "    b_t = np.array([p[0] for _, p in params_doremi[steps].items()])\n",
    "\n",
    "    A_doremi = A_doremi * b_t[:, np.newaxis, np.newaxis]\n",
    "\n",
    "    return A_doremi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_skillit_matrices(slice_list, seed, task_name):\n",
    "    A = np.load(f\"../skillit_graphs/{task_name}_{'_'.join(slice_list)}_normalized_seed_{seed}.npy\")\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_A_matrix_skillit_3(slice_list, df_break, task_name, params_skillit, steps, n_seeds):\n",
    "    all_matrices = []\n",
    "    for seed in range(n_seeds):\n",
    "        A_skillit = load_skillit_matrices(slice_list, seed, task_name).T\n",
    "\n",
    "        df_break_subset = df_break[(df_break.seed == seed) & (df_break.break_steps == steps)]\n",
    "        losses = df_break_subset.loc[df_break_subset.index.max()]['loss'].values.reshape(-1, 3)\n",
    "        b_t = params_skillit[steps][seed][0]\n",
    "\n",
    "        losses = losses.reshape(-1, 1)\n",
    "        all_matrices.append(A_skillit * losses * b_t)\n",
    "\n",
    "    return np.array(all_matrices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def compute_metric(A_star, A_hat):\n",
    "    metric = []\n",
    "    spearman = []\n",
    "    l2 = []\n",
    "    for a, b in zip(A_star, A_hat):\n",
    "        a_sum = a.sum(axis=0)\n",
    "        b_sum = b.sum(axis=0)\n",
    "\n",
    "        a_sum = a_sum / np.linalg.norm(a_sum)\n",
    "        b_sum = b_sum / np.linalg.norm(b_sum)\n",
    "        \n",
    "        cosim = cosine_similarity(a_sum.reshape(1, -1), b_sum.reshape(1, -1))[0]\n",
    "\n",
    "        rank_corr, _ = spearmanr(a_sum, b_sum)\n",
    "\n",
    "        l2.append(cosim)\n",
    "        spearman.append(rank_corr)\n",
    "        metric.append((cosim)* 0.5 + (rank_corr)*0.5)\n",
    "\n",
    "    metric = np.array(metric)\n",
    "    spearman = np.array(spearman)\n",
    "    print(spearman)\n",
    "    l2 = np.array(l2)\n",
    "    print(l2)\n",
    "    print(f\"Spearman: {spearman.mean()}\")\n",
    "    print(f\"L2: {l2.mean()}\")\n",
    "    print(f\"Metric: {metric.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arxiv / Book / Stackexchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_list = ['arxiv', 'books', 'stackexchange']\n",
    "\n",
    "\n",
    "params_doge_opt = load_params(slice_list, \"params_doge_trajectory_opt_1000.pkl\")\n",
    "\n",
    "slice_list = ['arxiv', 'book', 'stackexchange']\n",
    "\n",
    "A_doge_opt = get_A_matrix_dynamic_3(params_doge_opt, slice_list, 1000, 5)\n",
    "\n",
    "dirs = [\"../output/09242024/\", \"../output/09252024/\"]\n",
    "\n",
    "slice_list = ['arxiv', 'books', 'stackexchange']\n",
    "\n",
    "params_doge = load_params(slice_list, \"params_doge_trajectory_doge_matrix_1000.pkl\")\n",
    "\n",
    "slice_list = ['arxiv', 'book', 'stackexchange']\n",
    "\n",
    "A_doge = get_A_matrix_doge_3(slice_list, params_doge, 1000, dirs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metric(A_doge_opt, A_doge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_list = ['arxiv', 'books', 'stackexchange']\n",
    "\n",
    "\n",
    "params_doremi_opt = load_params(slice_list, \"params_doremi_trajectory_opt_1000_steps_500.pkl\")\n",
    "\n",
    "slice_list = ['arxiv', 'book', 'stackexchange']\n",
    "\n",
    "A_doremi_opt = get_A_matrix_dynamic_3(params_doremi_opt, slice_list, 1000, 5)\n",
    "\n",
    "dirs = [\"../output/09262024/\", \"../output/09252024/\"]\n",
    "\n",
    "slice_list = ['arxiv', 'books', 'stackexchange']\n",
    "\n",
    "params_doremi = load_params(slice_list, \"params_doremi_trajectory_doremi_matrix_1000_steps_500.pkl\")\n",
    "\n",
    "slice_list = ['arxiv', 'book', 'stackexchange']\n",
    "\n",
    "A_doremi = get_A_matrix_drm_3(slice_list, params_doremi, 1000, dirs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metric(A_doremi_opt, A_doremi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_list = ['arxiv', 'books', 'stackexchange']\n",
    "\n",
    "\n",
    "params_skillit_opt = load_params(slice_list, \"params_skillit_trajectory_opt_1000.pkl\")\n",
    "\n",
    "slice_list = ['arxiv', 'book', 'stackexchange']\n",
    "\n",
    "A_skillit_opt = get_A_matrix_dynamic_3(params_skillit_opt, slice_list, 1000, 5)\n",
    "\n",
    "dirs = [\"../output/09262024/\", \"../output/09272024/\"]\n",
    "df_break = load_break_losses(dirs, slice_list, 'slimpj', filter_word=\"greedy\")\n",
    "\n",
    "slice_list = ['arxiv', 'books', 'stackexchange']\n",
    "\n",
    "params_skillit = load_params(slice_list, \"params_skillit_trajectory_skillit_matrix_1000.pkl\")\n",
    "\n",
    "slice_list = ['arxiv', 'book', 'stackexchange']\n",
    "\n",
    "A_skillit = get_A_matrix_skillit_3(slice_list, df_break, 'slimpj', params_skillit, 1000, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metric(A_skillit_opt, A_skillit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_list = ['arxiv', 'books', 'stackexchange']\n",
    "\n",
    "\n",
    "params_aioli_opt = load_params(slice_list, \"params_aioli_trajectory_opt_1500.pkl\")\n",
    "\n",
    "slice_list = ['arxiv', 'book', 'stackexchange']\n",
    "\n",
    "A_aioli_opt = get_A_matrix_dynamic_3(params_aioli_opt, slice_list, 1500, 5)\n",
    "\n",
    "dirs = [\"../output/09272024/\", \"../output/09282024/\", \"../output/09292024/\"]\n",
    "\n",
    "slice_list = ['arxiv', 'books', 'stackexchange']\n",
    "\n",
    "params_aioli = load_params(slice_list, \"params_aioli_trajectory_aioli_matrix_1500.pkl\")\n",
    "\n",
    "slice_list = ['arxiv', 'book', 'stackexchange']\n",
    "\n",
    "A_aioli = get_A_matrix_aioli_3(slice_list, params_aioli, 1500, dirs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metric(A_aioli_opt, A_aioli)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mayeeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
