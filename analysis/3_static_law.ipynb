{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse \n",
    "import pandas as pd \n",
    "import os \n",
    "import numpy as np\n",
    "import torch \n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "from law import ScalingLaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_losses(dirs, slice_list, task_name, filter_word=None, exclude_word=None):\n",
    "    files_to_exclude = ['trace', 'val_inputs', 'labels', 'proportions', 'emb', 'rouge', 'generations', 'gradient', 'acc']    \n",
    "    df_all = pd.DataFrame() \n",
    "\n",
    "    for dir in dirs:\n",
    "        files = os.listdir(dir)\n",
    "        files = [os.path.join(dir, f) for f in files]\n",
    "        files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "\n",
    "        for file in files:\n",
    "            if \".log\" in file or task_name not in file:\n",
    "                continue \n",
    "\n",
    "            if any(skill not in file for skill in slice_list):\n",
    "                continue \n",
    "\n",
    "            method = file.split(\"/\")[-1]\n",
    "\n",
    "            if filter_word is not None and filter_word not in method:\n",
    "                continue\n",
    "\n",
    "            if exclude_word is not None and exclude_word in method:\n",
    "                continue\n",
    "           \n",
    "            if \"stratified\" in file or \"doge\" in file or \"skillit\" in file or \"aioli\" in file:\n",
    "                continue \n",
    "            \n",
    "\n",
    "            weight_str = method.split(\"weights_\")[-1].split(\"_\")[0]\n",
    "\n",
    "            if len(weight_str) == 3:\n",
    "                a = int(weight_str[0])\n",
    "                b = int(weight_str[1])\n",
    "                c = int(weight_str[2])\n",
    "            elif len(weight_str) == 4:\n",
    "                if weight_str == \"0010\":\n",
    "                    a = 0\n",
    "                    b = 0\n",
    "                    c = 10\n",
    "                elif weight_str == \"0100\":\n",
    "                    a = 0\n",
    "                    b = 10 \n",
    "                    c = 0\n",
    "                elif weight_str == \"1000\":\n",
    "                    a = 10 \n",
    "                    b = 0 \n",
    "                    c = 0\n",
    "            else:\n",
    "                a, b, c = [float(f\"0.{weight}\") for weight in weight_str.split(\"0.\")[1:]]\n",
    "\n",
    "            print(a, b, c)\n",
    "\n",
    "\n",
    "            runs = os.listdir(file)\n",
    "            for run in runs:\n",
    "                if \"test_\" in run:\n",
    "                    continue\n",
    "\n",
    "                if any([exclude_file in run for exclude_file in files_to_exclude]):\n",
    "                    continue \n",
    "\n",
    "                seed = int(run.split(\"seed_\")[-1].split(\"_\")[0])\n",
    "                checkpoint = int(run.split(\"-\")[-1].split(\".\")[0])\n",
    "\n",
    "\n",
    "                path = os.path.join(file, run)\n",
    "\n",
    "                df = pd.read_pickle(path)\n",
    "                df = df.rename(columns={\"task_idx\": \"skill\", \"task_loss\": \"loss\"})\n",
    "\n",
    "                df[\"method\"] = method\n",
    "                df[\"seed\"] = seed\n",
    "                df[\"checkpoint\"] = checkpoint\n",
    "                df[\"p1\"] = a \n",
    "                df[\"p2\"] = b\n",
    "                df[\"p3\"] = c\n",
    "\n",
    "\n",
    "                df.set_index(\"checkpoint\", inplace=True)\n",
    "\n",
    "\n",
    "                df_all = pd.concat([df_all, df])\n",
    "\n",
    "\n",
    "    df_all = df_all.sort_values(by=[\"checkpoint\", \"p1\", \"p2\", \"seed\"])\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The math of this mixing law\n",
    "\n",
    "The generic form reported in the paper is $L_i(p) = c_i + k_i \\exp(\\sum_{j=1}^m A_{ij} p_j)$. For $m=3$, we only need two parameters per row in $A$, so we can rewrite as\n",
    "\\begin{align}\n",
    "L_i(p) &= c_i + \\exp(\\log k_i + A_{i1} p_1 + A_{i2} p_2 + A_{i3} (1 - p_1 - p_2))  \\\\\n",
    "&= c_i + \\exp(\\log k_i + A_{i3} + (A_{i1} - A_{i3}) p_1 + (A_{i2} - A_{i3}) p_2) \\\\\n",
    "&= c_i + \\exp(b_i + t_{i1}p_1 + t_{i2}p_2)\n",
    "\\end{align}\n",
    "\n",
    "where $b_i = \\log k_i + A_{i3}$, $t_{i1} = A_{i1} - A_{i3}, t_{i2} = A_{i2} - A_{i3}$.\n",
    "\n",
    "That is, each regression involves 4 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixing_law(x, param):\n",
    "    log_c_i, b_i = param[0], param[1]\n",
    "    t_i = param[2:]\n",
    "    result = torch.exp(log_c_i) + torch.exp(b_i + torch.matmul(x[:, :2], t_i))\n",
    "    return result\n",
    "\n",
    "def init_params_law(idx, num_domains=3):\n",
    "    for log_c_i in np.linspace(-2, 1.5, 10):\n",
    "        for b_i in np.linspace(-10, 1, 20):\n",
    "            for _ in range(30):\n",
    "                ts = [-np.random.rand() if i == idx else np.random.rand() * 0.1 for i in range(num_domains-1)]\n",
    "                yield [log_c_i, b_i] + ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r_squared(actuals, predictions):\n",
    "    actuals, predictions = actuals.numpy(), predictions.numpy()\n",
    "    # Calculate the total sum of squares\n",
    "    total_sum_of_squares = np.sum((actuals - np.mean(actuals)) ** 2)\n",
    "    # Calculate the residual sum of squares\n",
    "    residual_sum_of_squares = np.sum((actuals - predictions) ** 2)\n",
    "    # Calculate R-squared\n",
    "    r_squared = 1 - (residual_sum_of_squares / total_sum_of_squares)\n",
    "    return r_squared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arxiv, Books, Stackexchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [\"../output/09032024/\", \n",
    "        \"../output/09042024/\",\n",
    "        \"../output/09052024/\",\n",
    "        \"../output/09062024/\"] # REPLACE WITH YOUR RUN OUTPUT DIRECTORIES\n",
    "task_name = \"slimpj\"\n",
    "slice_list = [\"arxiv\", \"book\", \"stackexchange\"]\n",
    "df = load_losses(dirs, slice_list, task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down the results to be only from the sweep\n",
    "grid = 9\n",
    "all_weights = []\n",
    "for seed in range(5):\n",
    "    with open(f\"../dirichlet_weights/k_3_n_{grid}_seed_{seed}.txt\", \"r\") as f:\n",
    "        all_weights.extend(f.readlines())\n",
    "\n",
    "all_weights = [weight_line.strip().replace(\",\", \"\") for weight_line in all_weights]\n",
    "all_weights = [weight_line.rstrip(\"0\") for weight_line in all_weights]\n",
    "while \"00.\" in \"\".join(all_weights):\n",
    "    all_weights = [weight_line.replace(\"00.\", \"0.\") for weight_line in all_weights]\n",
    "\n",
    "df = df[df.method.str.contains(\"|\".join(all_weights))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = sorted(df.skill.unique())\n",
    "indices = [5000] # end of training\n",
    "\n",
    "seeds = sorted(df.seed.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {skill : {idx: {seed: {} for seed in seeds} for idx in indices} for skill in skills }\n",
    "\n",
    "mses = []\n",
    "r2s = []\n",
    "\n",
    "for i, skill in enumerate(skills):\n",
    "    for index in indices:\n",
    "        for seed in seeds:\n",
    "            print(f\"Skill = {skill}, index = {index}, seed = {seed}\")\n",
    "\n",
    "            df_subset = df.loc[df.index == index]\n",
    "            df_subset = df_subset[(df_subset.skill == skill) & (df_subset.seed == seed)]\n",
    "\n",
    "            x = df_subset[['p1', 'p2', 'p3']].values \n",
    "\n",
    "            y = df_subset['loss'].values\n",
    "                \n",
    "            law = ScalingLaw(mixing_law)\n",
    "\n",
    "            p = law.fit(x, y, init_params_law(i, num_domains=len(skills)), max_step=100, delta=0.02)\n",
    "            params[skill][index][seed] = p\n",
    "\n",
    "            prediction_train = mixing_law(torch.tensor(x, dtype=torch.float), torch.tensor(p, dtype=torch.float))\n",
    "            rmse_train = (torch.mean((prediction_train - y)**2)**0.5).item()\n",
    "            mae_train = torch.mean(torch.abs(prediction_train - y)).item()\n",
    "            mse_train = torch.nn.functional.mse_loss(prediction_train, torch.tensor(y, dtype=torch.float)).item()\n",
    "            r2_train = calculate_r_squared(torch.tensor(y), torch.tensor(prediction_train))\n",
    "\n",
    "            mses.append(mse_train)\n",
    "            r2s.append(r2_train)\n",
    "\n",
    "\n",
    "            print(f\"RMSE: {rmse_train}, MAE: {mae_train}, MSE: {mse_train}, R2: {r2_train}\")\n",
    "\n",
    "mses = np.array(mses)\n",
    "r2s = np.array(r2s)\n",
    "\n",
    "print(mses.mean(), mses.std())\n",
    "print(r2s.mean(), r2s.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"./law_results/arxiv_books_stackexchange/params_static.pkl\", \"wb\") as f:\n",
    "    pickle.dump(params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get optimal mixture according to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "all_ps = []\n",
    "step = 500\n",
    "\n",
    "for seed in range(5):\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    b = torch.tensor([values[5000][seed][1] for _, values in params.items()])\n",
    "    t1 = torch.tensor([values[5000][seed][2] for _, values in params.items()])\n",
    "    t2 = torch.tensor([values[5000][seed][3] for _, values in params.items()])\n",
    "\n",
    "    # Initialize p1 and p2 with random values in [0, 1]\n",
    "    p1 = torch.rand(1, requires_grad=True)\n",
    "    p2 = torch.rand(1, requires_grad=True)\n",
    "\n",
    "    # Define the objective function\n",
    "    def objective(p1, p2):\n",
    "        return torch.sum(torch.exp(b + t1 * p1 + t2 * p2))\n",
    "\n",
    "    # Define a function to project onto the constraint set\n",
    "    def project(p1, p2):\n",
    "        p1.data.clamp_(0, 1)\n",
    "        p2.data.clamp_(0, 1)\n",
    "        sum_p = p1 + p2\n",
    "        if sum_p > 1:\n",
    "            factor = 1 / sum_p\n",
    "            p1.data.mul_(factor)\n",
    "            p2.data.mul_(factor)\n",
    "        return p1, p2\n",
    "\n",
    "    # Optimization\n",
    "    optimizer = optim.Adam([p1, p2], lr=0.001)\n",
    "    n_iterations = 10000\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        loss = objective(p1, p2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Project onto the constraint set\n",
    "        with torch.no_grad():\n",
    "            p1, p2 = project(p1, p2)\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Iteration {i+1}/{n_iterations}, Loss: {loss.item():.4f}, p1: {p1.item():.4f}, p2: {p2.item():.4f}')\n",
    "\n",
    "    print(f'Final result: p = {p1.item()}, {p2.item()}, {1-p1.item()-p2.item()}')\n",
    "\n",
    "    all_ps.append([p1.item(), p2.item(), 1 - p1.item() - p2.item()])\n",
    "    print(f'Final objective value: {objective(p1, p2).item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_ps) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mayeeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
