{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "from law import ScalingLaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_break_losses(dirs, slice_list, task_name):\n",
    "    files_to_exclude = ['trace', 'val_inputs', 'labels', 'proportions', 'emb', 'rouge', 'generations', 'gradient', 'acc']    \n",
    "    df_all = pd.DataFrame() \n",
    "\n",
    "    for dir in dirs:\n",
    "        files = os.listdir(dir)\n",
    "        files = [os.path.join(dir, f) for f in files] \n",
    "        files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "\n",
    "        for file in files:\n",
    "            if \".log\" in file or task_name not in file:\n",
    "                continue \n",
    "\n",
    "            if any(skill not in file for skill in slice_list):\n",
    "                continue \n",
    "\n",
    "            if \"break\" not in file:\n",
    "                continue\n",
    "\n",
    "\n",
    "            break_steps = int(file.split(\"break_\")[-1].split(\"_\")[0])\n",
    "           \n",
    "            method = file.split(\"/\")[-1]\n",
    "\n",
    "            weight_str = method.split(\"weights_\")[-1].split(\"_\")[0]\n",
    "            if len(weight_str) == 9:\n",
    "                a = int(weight_str[0])\n",
    "                b = int(weight_str[1])\n",
    "                c = int(weight_str[2])\n",
    "                d = int(weight_str[3])\n",
    "                e = int(weight_str[4])\n",
    "                f = int(weight_str[5])\n",
    "                g = int(weight_str[6])\n",
    "                h = int(weight_str[7])\n",
    "                i = int(weight_str[8])\n",
    "            else:\n",
    "                a, b, c, d, e, f, g, h, i = [float(f\"0.{weight}\") for weight in weight_str.split(\"0.\")[1:]]\n",
    "\n",
    "            print(a, b, c, d, e, f, g, h, i)\n",
    "\n",
    "\n",
    "            runs = os.listdir(file)\n",
    "            for run in runs:\n",
    "\n",
    "                if \"test_\" in run:\n",
    "                    continue\n",
    "\n",
    "                if any([exclude_file in run for exclude_file in files_to_exclude]):\n",
    "                    continue \n",
    "\n",
    "                seed = int(run.split(\"seed_\")[-1].split(\"_\")[0])\n",
    "                checkpoint = int(run.split(\"-\")[-1].split(\".\")[0])\n",
    "\n",
    "\n",
    "                path = os.path.join(file, run)\n",
    "\n",
    "                df = pd.read_pickle(path)\n",
    "\n",
    "                df = df.rename(columns={\"task_idx\": \"skill\", \"task_loss\": \"loss\"})\n",
    "\n",
    "                df[\"method\"] = method\n",
    "                df[\"seed\"] = seed\n",
    "                df[\"checkpoint\"] = checkpoint\n",
    "                df[\"break_steps\"] = break_steps\n",
    "                df[\"p1\"] = a \n",
    "                df[\"p2\"] = b\n",
    "                df[\"p3\"] = c\n",
    "                df[\"p4\"] = d\n",
    "                df[\"p5\"] = e\n",
    "                df[\"p6\"] = f\n",
    "                df[\"p7\"] = g\n",
    "                df[\"p8\"] = h\n",
    "                df[\"p9\"] = i\n",
    "\n",
    "\n",
    "                df.set_index(\"checkpoint\", inplace=True)\n",
    "\n",
    "\n",
    "                df_all = pd.concat([df_all, df])\n",
    "\n",
    "\n",
    "    df_all = df_all.sort_values(by=[\"checkpoint\", \"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\", \"p7\", \"p8\", \"p9\", \"seed\"])\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resume_losses(dirs, slice_list, task_name):\n",
    "    files_to_exclude = ['trace', 'val_inputs', 'labels', 'proportions', 'emb', 'rouge', 'generations', 'gradient', 'acc']    \n",
    "    df_all = pd.DataFrame() \n",
    "\n",
    "    for dir in dirs:\n",
    "        files = os.listdir(dir)\n",
    "        files = [os.path.join(dir, f) for f in files] # add path to each file\n",
    "        files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "\n",
    "        for file in files:\n",
    "            if \".log\" in file or task_name not in file:\n",
    "                continue \n",
    "\n",
    "            if any(skill not in file for skill in slice_list):\n",
    "                continue \n",
    "\n",
    "            if \"resume\" not in file:\n",
    "                continue\n",
    "\n",
    "\n",
    "            old_weight_str = file.split(\"resume_\")[-1].split(\"_\")[0]\n",
    "            print(file)\n",
    "            if len(old_weight_str) == 9:\n",
    "                a = int(old_weight_str[0])\n",
    "                b = int(old_weight_str[1])\n",
    "                c = int(old_weight_str[2])\n",
    "                d = int(old_weight_str[3])\n",
    "                e = int(old_weight_str[4])\n",
    "                f = int(old_weight_str[5])\n",
    "                g = int(old_weight_str[6])\n",
    "                h = int(old_weight_str[7])\n",
    "                i = int(old_weight_str[8])\n",
    "            else:\n",
    "                if len(old_weight_str.split(\"0.\")) == 2:\n",
    "                    continue \n",
    "                a, b, c, d, e, f, g, h, i = [float(f\"0.{weight}\") for weight in old_weight_str.split(\"0.\")[1:]]\n",
    "\n",
    "            print(a, b, c, d, e, f, g, h, i)\n",
    "\n",
    "\n",
    "            method = file.split(\"/\")[-1]\n",
    "\n",
    "            new_weight_str = method.split(\"weights_\")[-1].split(\"_\")[0]\n",
    "            if len(new_weight_str) == 9:\n",
    "                new_a = int(new_weight_str[0])\n",
    "                new_b = int(new_weight_str[1])\n",
    "                new_c = int(new_weight_str[2])\n",
    "                new_d = int(new_weight_str[3])\n",
    "                new_e = int(new_weight_str[4])\n",
    "                new_f = int(new_weight_str[5])\n",
    "                new_g = int(new_weight_str[6])\n",
    "                new_h = int(new_weight_str[7])\n",
    "                new_i = int(new_weight_str[8])\n",
    "            else:\n",
    "                new_a, new_b, new_c, new_d, new_e, new_f, new_g, new_h, new_i = [float(f\"0.{weight}\") for weight in new_weight_str.split(\"0.\")[1:]]\n",
    "\n",
    "\n",
    "            break_steps = int(file.split(f\"resume_{old_weight_str}_\")[-1].split(\"_\")[0])\n",
    "\n",
    "\n",
    "            runs = os.listdir(file)\n",
    "            for run in runs:\n",
    "\n",
    "                if \"test_\" in run:\n",
    "                    continue\n",
    "\n",
    "                if any([exclude_file in run for exclude_file in files_to_exclude]):\n",
    "                    continue \n",
    "\n",
    "                seed = int(run.split(\"seed_\")[-1].split(\"_\")[0])\n",
    "                checkpoint = int(run.split(\"-\")[-1].split(\".\")[0])\n",
    "\n",
    "\n",
    "                path = os.path.join(file, run)\n",
    "\n",
    "                df = pd.read_pickle(path)\n",
    "\n",
    "                df = df.rename(columns={\"task_idx\": \"skill\", \"task_loss\": \"loss\"})\n",
    "\n",
    "                df[\"method\"] = method\n",
    "                df[\"seed\"] = seed\n",
    "                df[\"checkpoint\"] = checkpoint\n",
    "                df[\"break_steps\"] = break_steps\n",
    "                df[\"new_p1\"] = new_a \n",
    "                df[\"new_p2\"] = new_b \n",
    "                df[\"new_p3\"] = new_c \n",
    "                df[\"new_p4\"] = new_d \n",
    "                df[\"new_p5\"] = new_e \n",
    "                df[\"new_p6\"] = new_f \n",
    "                df[\"new_p7\"] = new_g \n",
    "                df[\"new_p8\"] = new_h \n",
    "                df[\"new_p9\"] = new_i \n",
    "\n",
    "                df[\"p1\"] = a \n",
    "                df[\"p2\"] = b\n",
    "                df[\"p3\"] = c\n",
    "                df[\"p4\"] = d\n",
    "                df[\"p5\"] = e\n",
    "                df[\"p6\"] = f\n",
    "                df[\"p7\"] = g\n",
    "                df[\"p8\"] = h\n",
    "                df[\"p9\"] = i\n",
    "\n",
    "\n",
    "                df.set_index(\"checkpoint\", inplace=True)\n",
    "\n",
    "\n",
    "                df_all = pd.concat([df_all, df])\n",
    "\n",
    "\n",
    "    df_all = df_all.sort_values(by=[\"checkpoint\", \"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\", \"p7\", \"p8\", \"p9\", \"new_p1\", \"new_p2\", \"new_p3\", \"new_p4\", \"new_p5\", \"new_p6\", \"new_p7\", \"new_p8\", \"new_p9\", \"seed\"])\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r_squared(actuals, predictions):\n",
    "    actuals, predictions = actuals.numpy(), predictions.numpy()\n",
    "    # Calculate the total sum of squares\n",
    "    total_sum_of_squares = np.sum((actuals - np.mean(actuals)) ** 2)\n",
    "    # Calculate the residual sum of squares\n",
    "    residual_sum_of_squares = np.sum((actuals - predictions) ** 2)\n",
    "    # Calculate R-squared\n",
    "    r_squared = 1 - (residual_sum_of_squares / total_sum_of_squares)\n",
    "    return r_squared\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixing_law(x, param):\n",
    "    # one set of params per skill\n",
    "    #print(param)\n",
    "    c_i = param[0]\n",
    "    t_i = param[1:]\n",
    "    result = c_i + torch.matmul(x[:, :8], t_i)\n",
    "    return result\n",
    "\n",
    "def init_params_law(idx, num_domains=9):\n",
    "    for c_i in np.linspace(0.5, 5, 10):\n",
    "        for _ in range(30):\n",
    "            ts = [-np.random.rand() if i == idx else np.random.rand() * 0.1 for i in range(num_domains-1)]\n",
    "            yield [c_i] + ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_individual_xy(df_break, df_resume, skill, break_steps, p1):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    df_break_subset = df_break[(df_break.break_steps == break_steps) & (df_break.p1 == p1) & (df_break.skill == skill)]\n",
    "    df_break_subset = df_break_subset.loc[df_break_subset.index.max()]\n",
    "\n",
    "    df_resume_subset = df_resume[(df_resume.break_steps == break_steps) & (df_resume.p1 == p1) & (df_resume.skill == skill)]\n",
    "    df_resume_subset = df_resume_subset.loc[df_resume_subset.index.max()]\n",
    "    \n",
    "        \n",
    "    x = df_resume_subset[['new_p1', 'new_p2', 'new_p3', 'new_p4', 'new_p5', 'new_p6', 'new_p7', 'new_p8', 'new_p9']].values\n",
    "    y = df_resume_subset['loss'].values\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_individual_xy_log(df_break, df_resume, skill, break_steps, p1):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    df_break_subset = df_break[(df_break.break_steps == break_steps) & (df_break.p1 == p1) & (df_break.skill == skill)]\n",
    "    df_break_subset = df_break_subset.loc[df_break_subset.index.max()]\n",
    "\n",
    "    df_resume_subset = df_resume[(df_resume.break_steps == break_steps) & (df_resume.p1 == p1) & (df_resume.skill == skill)]\n",
    "    df_resume_subset = df_resume_subset.loc[df_resume_subset.index.max()]\n",
    "    \n",
    "        \n",
    "    x = df_resume_subset[['new_p1', 'new_p2', 'new_p3', 'new_p4', 'new_p5', 'new_p6', 'new_p7', 'new_p8', 'new_p9']].values\n",
    "\n",
    "    L0 = df_break_subset['loss']\n",
    "\n",
    "    y = np.log(df_resume_subset['loss'].values / L0)\n",
    "\n",
    "    return x, y, L0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [\"../output/11062024/\", \"../output/10292024/\", \"../output/10302024/\", \"../output/10312024/\"] # REPLACE WITH YOUR RUN OUTPUT DIRECTORIES\n",
    "task_name = \"instruction\"\n",
    "slice_list = [\"\"]\n",
    "df_break = load_break_losses(dirs, slice_list, task_name)\n",
    "df_break = df_break[df_break.seed==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = sorted(df_break.skill.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume = load_resume_losses(dirs, slice_list, task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "break_steps = sorted(df_resume.break_steps.unique())\n",
    "probs = sorted(df_break.p1.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {skill : {bs: {p: {} for p in probs} for bs in break_steps} for skill in skills }\n",
    "\n",
    "x_per_skill = {skill : {bs: {p: {} for p in probs} for bs in break_steps} for skill in skills }\n",
    "y_per_skill = {skill : {bs: {p: {} for p in probs} for bs in break_steps} for skill in skills }\n",
    "\n",
    "mses = []\n",
    "r2s = []\n",
    "for i, skill in enumerate(skills):\n",
    "    for bs in break_steps:\n",
    "        for p1 in probs:\n",
    "            print(skill, bs, p1)\n",
    "            x, y = make_individual_xy(df_break, df_resume, skill, bs, p1)\n",
    "            \n",
    "            x_per_skill[skill][bs][p1] = x\n",
    "            y_per_skill[skill][bs][p1] = y\n",
    "                        \n",
    "            law = ScalingLaw(mixing_law)\n",
    "            p = law.fit(x, y, init_params_law(i, num_domains=len(skills)), max_step=100, delta=0.02)\n",
    "            params[skill][bs][p1] = p # param\n",
    "\n",
    "            prediction_train = mixing_law(torch.tensor(x, dtype=torch.float), torch.tensor(p, dtype=torch.float))\n",
    "            mse_train = torch.nn.functional.mse_loss(prediction_train, torch.tensor(y, dtype=torch.float)).item()\n",
    "            r2_train = calculate_r_squared(torch.tensor(y), torch.tensor(prediction_train))\n",
    "\n",
    "            mses.append(mse_train)\n",
    "            r2s.append(r2_train)\n",
    "\n",
    "\n",
    "            print(f\"MSE: {mse_train}, R2: {r2_train}\")\n",
    "\n",
    "\n",
    "mses = np.array(mses)\n",
    "r2s = np.array(r2s)\n",
    "\n",
    "print(mses.mean(), mses.std())\n",
    "print(r2s.mean(), r2s.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "x_per_skill = {skill : {bs: {p: {} for p in probs} for bs in break_steps} for skill in skills }\n",
    "y_per_skill = {skill : {bs: {p: {} for p in probs} for bs in break_steps} for skill in skills }\n",
    "\n",
    "mses = []\n",
    "r2s = []\n",
    "\n",
    "mse_per_skill = defaultdict(list)\n",
    "r2_per_skill = defaultdict(list)\n",
    "\n",
    "for i, skill in enumerate(skills):\n",
    "    for bs in break_steps:\n",
    "        for p1 in probs:\n",
    "            x, y = make_individual_xy(df_break, df_resume, skill, bs, p1)\n",
    "            \n",
    "            x_per_skill[skill][bs][p1] = x\n",
    "            y_per_skill[skill][bs][p1] = y\n",
    "                        \n",
    "            law = ScalingLaw(mixing_law)\n",
    "            p = params[skill][bs][p1]\n",
    "\n",
    "            prediction_train = mixing_law(torch.tensor(x, dtype=torch.float), torch.tensor(p, dtype=torch.float))\n",
    "            mse_train = torch.nn.functional.mse_loss(prediction_train, torch.tensor(y, dtype=torch.float)).item()\n",
    "            r2_train = calculate_r_squared(torch.tensor(y), torch.tensor(prediction_train))\n",
    "\n",
    "            mses.append(mse_train)\n",
    "            r2s.append(r2_train)\n",
    "\n",
    "\n",
    "            mse_per_skill[skill].append(mse_train)\n",
    "            r2_per_skill[skill].append(r2_train)\n",
    "\n",
    "\n",
    "mses = np.array(mses)\n",
    "r2s = np.array(r2s)\n",
    "\n",
    "print(mses.mean(), mses.std())\n",
    "print(r2s.mean(), r2s.std())\n",
    "print(\"\\n\")\n",
    "for skill in skills:\n",
    "    print(skill)\n",
    "    mses = np.array(mse_per_skill[skill])\n",
    "    r2s = np.array(r2_per_skill[skill])\n",
    "\n",
    "    print(mses.mean(), mses.std())\n",
    "    print(r2s.mean(), r2s.std())\n",
    "\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"./law_results/instruction/params_dynamic.pkl\", \"wb\") as f:\n",
    "    pickle.dump(params, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_break = df_break[df_break.seed==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_log = {skill : {bs: {p: {} for p in probs} for bs in break_steps} for skill in skills }\n",
    "\n",
    "x_per_skill = {skill : {bs: {p: {} for p in probs} for bs in break_steps} for skill in skills }\n",
    "y_per_skill = {skill : {bs: {p: {} for p in probs} for bs in break_steps} for skill in skills }\n",
    "\n",
    "mses = []\n",
    "r2s = []\n",
    "for i, skill in enumerate(skills):\n",
    "    for bs in break_steps:\n",
    "        for p1 in probs:\n",
    "            print(skill, bs, p1)\n",
    "            x, y, _ = make_individual_xy_log(df_break, df_resume, skill, bs, p1)\n",
    "            \n",
    "            x_per_skill[skill][bs][p1] = x\n",
    "            y_per_skill[skill][bs][p1] = y\n",
    "                        \n",
    "            law = ScalingLaw(mixing_law)\n",
    "            p = law.fit(x, y, init_params_law(i, num_domains=len(skills)), max_step=100, delta=0.02)\n",
    "            params_log[skill][bs][p1] = p # param\n",
    "\n",
    "            prediction_train = mixing_law(torch.tensor(x, dtype=torch.float), torch.tensor(p, dtype=torch.float))\n",
    "            mse_train = torch.nn.functional.mse_loss(prediction_train, torch.tensor(y, dtype=torch.float)).item()\n",
    "            r2_train = calculate_r_squared(torch.tensor(y), torch.tensor(prediction_train))\n",
    "\n",
    "            mses.append(mse_train)\n",
    "            r2s.append(r2_train)\n",
    "\n",
    "\n",
    "            print(f\"MSE: {mse_train}, R2: {r2_train}\")\n",
    "\n",
    "\n",
    "mses = np.array(mses)\n",
    "r2s = np.array(r2s)\n",
    "\n",
    "print(mses.mean(), mses.std())\n",
    "print(r2s.mean(), r2s.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"./law_results/instruction/params_log_dynamic.pkl\", \"wb\") as f:\n",
    "    pickle.dump(params_log, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_per_skill = {skill : {bs: {p: {} for p in probs} for bs in break_steps} for skill in skills }\n",
    "y_per_skill = {skill : {bs: {p: {} for p in probs} for bs in break_steps} for skill in skills }\n",
    "\n",
    "mses = []\n",
    "r2s = []\n",
    "\n",
    "mse_per_skill = defaultdict(dict)\n",
    "r2_per_skill = defaultdict(dict)\n",
    "\n",
    "for i, skill in enumerate(skills):\n",
    "    for bs in break_steps:\n",
    "        for p1 in probs:\n",
    "            x, y, _ = make_individual_xy_log(df_break, df_resume, skill, bs, p1)\n",
    "            \n",
    "            x_per_skill[skill][bs][p1] = x\n",
    "            y_per_skill[skill][bs][p1] = y\n",
    "                        \n",
    "            p = params_log[skill][bs][p1]\n",
    "\n",
    "            prediction_train = mixing_law(torch.tensor(x, dtype=torch.float), torch.tensor(p, dtype=torch.float))\n",
    "            mse_train = torch.nn.functional.mse_loss(prediction_train, torch.tensor(y, dtype=torch.float)).item()\n",
    "            r2_train = calculate_r_squared(torch.tensor(y), torch.tensor(prediction_train))\n",
    "\n",
    "            mses.append(mse_train)\n",
    "            r2s.append(r2_train)\n",
    "\n",
    "\n",
    "\n",
    "            mse_per_skill[skill][p1] = mse_train\n",
    "            r2_per_skill[skill][p1] = r2_train\n",
    "\n",
    "\n",
    "\n",
    "mses = np.array(mses)\n",
    "r2s = np.array(r2s)\n",
    "\n",
    "print(mses.mean(), mses.std())\n",
    "print(r2s.mean(), r2s.std())\n",
    "\n",
    "print(\"\\n\")\n",
    "for skill in skills:\n",
    "    print(skill)\n",
    "    mses = np.array(list(mse_per_skill[skill].values()))\n",
    "    r2s = np.array(list(r2_per_skill[skill].values()))\n",
    "\n",
    "    print(mses.mean(), mses.std())\n",
    "    print(r2s.mean(), r2s.std())\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mayeeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
