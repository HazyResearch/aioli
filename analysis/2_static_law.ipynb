{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse \n",
    "import pandas as pd \n",
    "import os \n",
    "import numpy as np\n",
    "import torch \n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from law import ScalingLaw\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import r2_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_losses(dirs, slice_list, task_name, filter_word=None, run_subset=None, include_extrema=False):\n",
    "    files_to_exclude = ['trace', 'val_inputs', 'labels', 'proportions', 'emb', 'rouge', 'generations', 'gradient', 'acc']    \n",
    "    df_all = pd.DataFrame() \n",
    "\n",
    "    for dir in dirs:\n",
    "        files = os.listdir(dir)\n",
    "        files = [os.path.join(dir, f) for f in files]\n",
    "        files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "\n",
    "        for file in files:\n",
    "            if \".log\" in file or task_name not in file:\n",
    "                continue \n",
    "\n",
    "            if \"weights_\" not in file:\n",
    "                continue \n",
    "\n",
    "            if any(skill not in file for skill in slice_list):\n",
    "                continue \n",
    "\n",
    "            if filter_word is not None and filter_word not in file:\n",
    "                continue\n",
    "           \n",
    "\n",
    "            method = file.split(\"/\")[-1]\n",
    "\n",
    "            weight_str = method.split(\"weights_\")[-1].split(\"_\")[0]\n",
    "\n",
    "            if len(weight_str) == 2:\n",
    "                a = int(weight_str[0])\n",
    "                b = int(weight_str[1])\n",
    "            elif len(weight_str) == 3:\n",
    "                if weight_str[0] == \"0\":\n",
    "                    a = 0\n",
    "                    b = int(weight_str[1:])\n",
    "                elif weight_str[-1] == \"0\":\n",
    "                    b = 0 \n",
    "                    a = int(weight_str[:2])\n",
    "            else:\n",
    "                idx = [i for i, ltr in enumerate(weight_str) if ltr == \".\"][1] - 1\n",
    "                a = float(weight_str[:idx])\n",
    "                b = float(weight_str[idx:])\n",
    "\n",
    "            if not include_extrema:\n",
    "                if a == 0 or b == 0:\n",
    "                    continue\n",
    "\n",
    "            if a + b != 10:\n",
    "                continue \n",
    "\n",
    "            if run_subset is not None:\n",
    "                if a not in run_subset:\n",
    "                    continue\n",
    "\n",
    "            print(a, b)\n",
    "\n",
    "\n",
    "            runs = os.listdir(file)\n",
    "            for run in runs:\n",
    "\n",
    "                if \"test_\" in run:\n",
    "                    continue\n",
    "\n",
    "                if any([exclude_file in run for exclude_file in files_to_exclude]):\n",
    "                    continue \n",
    "\n",
    "                seed = int(run.split(\"seed_\")[-1].split(\"_\")[0])\n",
    "                checkpoint = int(run.split(\"-\")[-1].split(\".\")[0])\n",
    "\n",
    "\n",
    "                path = os.path.join(file, run)\n",
    "\n",
    "                df = pd.read_pickle(path)\n",
    "                df = df.rename(columns={\"task_idx\": \"skill\", \"task_loss\": \"loss\"})\n",
    "\n",
    "                df[\"method\"] = method\n",
    "                df[\"seed\"] = seed\n",
    "                df[\"checkpoint\"] = checkpoint\n",
    "                df[\"p1\"] = a \n",
    "                df[\"p2\"] = b\n",
    "\n",
    "\n",
    "                df.set_index(\"checkpoint\", inplace=True)\n",
    "\n",
    "\n",
    "                df_all = pd.concat([df_all, df])\n",
    "\n",
    "\n",
    "    df_all = df_all.sort_values(by=[\"checkpoint\", \"p1\", \"seed\"])\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mixing law math\n",
    "\n",
    "$L_i(p) = c_i + b_i \\exp(k_i p_i)$\n",
    "\n",
    "For $m=2$ domains, this can be derived from the more general $L_i(p) = c_i + b_i \\exp(\\sum_{j=1}^m A_{ij} p_j)$ (here it appears that there are 4 parameters per domain, but since $p$ is on the simplex, we only need 3 parameters as described in the equation above.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_law(x, param):\n",
    "    k, b = param\n",
    "    # y = c + exp(kx+b)\n",
    "    return torch.exp(k*x + b)\n",
    "\n",
    "def param_generator():\n",
    "    for k in np.linspace(-2.4, -1.6, 11):\n",
    "        for b in np.linspace(-1.0, -0.1, 11):\n",
    "            yield [k, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_individual_xy(df, skill, skills, break_steps):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    df_resume_subset = df[(df.index == break_steps) & (df.skill == skill)]\n",
    "    df_resume_subset = df_resume_subset.loc[df_resume_subset.index.max()]\n",
    "    \n",
    "    if skill == skills[0]:\n",
    "        p_col = 'p1_normalized'\n",
    "    else:\n",
    "        p_col = 'p2_normalized'\n",
    "        \n",
    "    x = df_resume_subset[p_col].values\n",
    "    y = df_resume_subset['loss'].values\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arxiv, Stackexchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_0_dirs = [\"../output/08232024/\", \"../output/08242024/\", \"../output/08252024/\", \"../output/08262024/\", \n",
    "        \"../output/08292024/\", \"../output/08302024/\"] # REPLACE WITH YOUR RUN OUTPUT DIRECTORIES\n",
    "\n",
    "task_name = \"slimpj\"\n",
    "slice_list = ['arxiv', 'stackexchange']\n",
    "df = load_losses(ckpt_0_dirs, slice_list, task_name, run_subset=[1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "df['p1_normalized'] = df.apply(lambda x: x['p1']/(x['p1'] + x['p2']), axis=1)\n",
    "df['p2_normalized'] = df.apply(lambda x: x['p2']/(x['p1'] + x['p2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = sorted(df.skill.unique())\n",
    "indices = [5000] # fit at end of training\n",
    "\n",
    "seeds = sorted(df.seed.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {skill: {index: {seed: {} for seed in seeds} for index in indices} for skill in skills}\n",
    "grid_size = 1000\n",
    "for skill in skills:\n",
    "    for index in indices: \n",
    "        for seed in seeds:\n",
    "            print(f\"Skill = {skill}, index = {index}, seed = {seed}\")\n",
    "\n",
    "            df_subset = df.loc[df.index == index]\n",
    "            df_subset = df_subset[(df_subset.skill == skill) & (df_subset.seed == seed)]\n",
    "            best_c, max_corr = 0, 0\n",
    "\n",
    "            if skill == slice_list[0]:\n",
    "                p_col = 'p1_normalized'\n",
    "            else:\n",
    "                p_col = 'p2_normalized'\n",
    "\n",
    "            x = df_subset[p_col].values \n",
    "\n",
    "            y = df_subset['loss'].values\n",
    "\n",
    "            for i, c in enumerate(np.linspace(0, df_subset.loss.min(), grid_size)):\n",
    "\n",
    "                if i == grid_size - 1:\n",
    "                    continue \n",
    "                corr = np.abs(np.corrcoef(x, np.log(y-c))[0, 1])\n",
    "                if corr > max_corr:\n",
    "                    max_corr = corr\n",
    "                    best_c = c \n",
    "                \n",
    "            law = ScalingLaw(inter_law)\n",
    "            p = law.fit(x, y-best_c, param_generator(), max_step=100, delta=0.02)\n",
    "            params[skill][index][seed] = [best_c, p[0], p[1]] # param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"./law_results/arxiv_stackexchange/params_static.pkl\", \"wb\") as f:\n",
    "    pickle.dump(params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"DeJavu Serif\"\n",
    "\n",
    "c = np.array([[31, 119, 180], [255, 137, 17], [214, 39, 40], [255, 152, 150], [227, 119, 194]])/255\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 4))\n",
    "fig.subplots_adjust(bottom=0.2, top=1)\n",
    "\n",
    "xs = np.linspace(0, 1, 20)\n",
    "\n",
    "\n",
    "indices = [5000]\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "\n",
    "mses = []\n",
    "r2s = [] \n",
    "\n",
    "for k, skill in enumerate(slice_list):\n",
    "    ax = axes[k]\n",
    "\n",
    "    for j, index in enumerate(indices):\n",
    "        for j, seed in enumerate(seeds):\n",
    "            if skill == slice_list[0]:\n",
    "                p_col = 'p1_normalized'\n",
    "            else:\n",
    "                p_col = 'p2_normalized'\n",
    "\n",
    "            df_subset = df.loc[df.index == index]\n",
    "            df_subset = df_subset[(df_subset.skill == skill) & (df_subset.seed == seed)]\n",
    "            x = df_subset[p_col].values \n",
    "            y = df_subset['loss'].values\n",
    "\n",
    "            p = params[skill][index][seed]\n",
    "\n",
    "            if j != 0:\n",
    "                ax.scatter(x, y-p[0], color=c[j], s=300, alpha=0.2)\n",
    "            else:\n",
    "                ax.scatter(x, y-p[0], color=c[j], s=300)\n",
    "\n",
    "            plot_preds = inter_law(torch.tensor(xs), torch.tensor(p[1:]))\n",
    "\n",
    "            if j != 0:\n",
    "                ax.plot(xs, plot_preds, color=c[j], lw=3, alpha=0.2)\n",
    "            else:\n",
    "                ax.plot(xs, plot_preds, color=c[j], lw=3)\n",
    "\n",
    "            eval_preds = inter_law(torch.tensor(x), torch.tensor(p[1:]))\n",
    "\n",
    "            mse = torch.nn.functional.mse_loss(eval_preds, torch.tensor(y-p[0])).item()\n",
    "            r2 = r2_score(eval_preds, torch.tensor(y-p[0]))\n",
    "\n",
    "            mses.append(mse)\n",
    "            r2s.append(r2)\n",
    "\n",
    "            print(f\"MSE for {skill}, step {index}: {mse}\")\n",
    "            print(f\"R2 score for {skill}, step {index}: {r2}\\n\")\n",
    "            \n",
    "            ax.set_yscale(\"log\")\n",
    "            ax.set_xlabel(f\"Proportion of {skill}\", fontsize=18)\n",
    "            skill_str = skill\n",
    "\n",
    "            ax.set_ylabel(f\"Log (Loss - c) on {skill}\", fontsize=15)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.spines['top'].set_visible(False)\n",
    "\n",
    "            ax.grid()\n",
    "legend_elements = []\n",
    "\n",
    "\n",
    "fig.subplots_adjust(bottom=0.1, top=0.85)  \n",
    "fig.suptitle('Log-linear static mixing law on Arxiv/StackExchange', fontsize=20)\n",
    "\n",
    "#plt.savefig('../figs/arxiv_stackexchange_static.pdf', bbox_inches=\"tight\")\n",
    "\n",
    "mses = np.array(mses)\n",
    "r2s = np.array(r2s)\n",
    "\n",
    "print(mses.mean(), mses.std())\n",
    "print(r2s.mean(), r2s.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get optimal parameters according to fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 5000\n",
    "for seed in range(5):\n",
    "    k1, b1 = params['arxiv'][t][seed][1], params['arxiv'][t][seed][2]\n",
    "    k2, b2 = params['stackexchange'][t][seed][1], params['stackexchange'][t][seed][2]\n",
    "\n",
    "    p1 = (k2 + b2 - b1 + np.log(k2 / k1)) / (k1 + k2) # closed form\n",
    "    print(p1, 1-p1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mayeeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
