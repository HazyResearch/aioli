{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse \n",
    "import pandas as pd \n",
    "import os \n",
    "import numpy as np\n",
    "import torch \n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from law import ScalingLaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_weights(weight_str):\n",
    "    if len(weight_str) == 2:\n",
    "        a = int(weight_str[0])\n",
    "        b = int(weight_str[1])\n",
    "    elif len(weight_str) == 3:\n",
    "        if weight_str[0] == \"0\":\n",
    "            a = 0\n",
    "            b = int(weight_str[1:])\n",
    "        elif weight_str[-1] == \"0\":\n",
    "            b = 0 \n",
    "            a = int(weight_str[:2])\n",
    "    else:\n",
    "        idx = [i for i, ltr in enumerate(weight_str) if ltr == \".\"][1] - 1\n",
    "        a = float(weight_str[:idx])\n",
    "        b = float(weight_str[idx:])\n",
    "    return a, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_break_losses(dirs, slice_list, task_name, filter_word=None, include_extrema=False):\n",
    "    files_to_exclude = ['trace', 'val_inputs', 'labels', 'proportions', 'emb', 'rouge', 'generations', 'gradient', 'acc']    \n",
    "    df_all = pd.DataFrame() \n",
    "\n",
    "    for dir in dirs:\n",
    "        files = os.listdir(dir)\n",
    "        files = [os.path.join(dir, f) for f in files] \n",
    "        files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "\n",
    "        for file in files:\n",
    "            if \".log\" in file or task_name not in file:\n",
    "                continue \n",
    "\n",
    "            if any(skill not in file for skill in slice_list):\n",
    "                continue \n",
    "\n",
    "            if \"break\" not in file:\n",
    "                continue\n",
    "\n",
    "            if \"weights_\" not in file:\n",
    "                continue\n",
    "\n",
    "            if filter_word is not None and filter_word not in file:\n",
    "                continue\n",
    "\n",
    "            break_steps = int(file.split(\"break_\")[-1].split(\"_\")[0])\n",
    "           \n",
    "            method = file.split(\"/\")[-1]\n",
    "            weight_str = method.split(\"weights_\")[-1].split(\"_\")[0]\n",
    "            a, b = parse_weights(weight_str)\n",
    "            if a + b != 10:\n",
    "                continue \n",
    "\n",
    "            if not include_extrema:\n",
    "                if a == 0 or b == 0:\n",
    "                    continue\n",
    "\n",
    "            print(a, b)\n",
    "\n",
    "            runs = os.listdir(file)\n",
    "            for run in runs:\n",
    "\n",
    "                if \"test_\" in run:\n",
    "                    continue\n",
    "\n",
    "                if any([exclude_file in run for exclude_file in files_to_exclude]):\n",
    "                    continue \n",
    "\n",
    "                seed = int(run.split(\"seed_\")[-1].split(\"_\")[0])\n",
    "                checkpoint = int(run.split(\"-\")[-1].split(\".\")[0])\n",
    "\n",
    "\n",
    "                path = os.path.join(file, run)\n",
    "\n",
    "                df = pd.read_pickle(path)\n",
    "\n",
    "                df = df.rename(columns={\"task_idx\": \"skill\", \"task_loss\": \"loss\"})\n",
    "\n",
    "                df[\"method\"] = method\n",
    "                df[\"seed\"] = seed\n",
    "                df[\"checkpoint\"] = checkpoint\n",
    "                df[\"break_steps\"] = break_steps\n",
    "                df[\"p1\"] = a \n",
    "                df[\"p2\"] = b\n",
    "\n",
    "\n",
    "                df.set_index(\"checkpoint\", inplace=True)\n",
    "\n",
    "\n",
    "                df_all = pd.concat([df_all, df])\n",
    "\n",
    "\n",
    "    df_all = df_all.sort_values(by=[\"checkpoint\", \"p1\", \"seed\"])\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resume_losses(dirs, slice_list, task_name, filter_word=None, include_extrema=False):\n",
    "    files_to_exclude = ['trace', 'val_inputs', 'labels', 'proportions', 'emb', 'rouge', 'generations', 'gradient', 'acc']    \n",
    "    df_all = pd.DataFrame() \n",
    "\n",
    "    for dir in dirs:\n",
    "        files = os.listdir(dir)\n",
    "        files = [os.path.join(dir, f) for f in files] # add path to each file\n",
    "        files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "\n",
    "        for file in files:\n",
    "            if \".log\" in file or task_name not in file:\n",
    "                continue \n",
    "\n",
    "            if any(skill not in file for skill in slice_list):\n",
    "                continue \n",
    "\n",
    "            if \"resume\" not in file:\n",
    "                continue\n",
    "            if \"_weights\" not in file:\n",
    "                continue\n",
    "\n",
    "            if filter_word is not None and filter_word not in file:\n",
    "                continue\n",
    "\n",
    "            new_weight_str = file.split(\"resume_\")[-1].split(\"_\")[0]\n",
    "            new_a, new_b = parse_weights(new_weight_str)\n",
    "\n",
    "            if new_a + new_b != 10:\n",
    "                continue \n",
    "            print(new_a, new_b)\n",
    "\n",
    "            if not include_extrema:\n",
    "                if new_a == 0 or new_b == 0:\n",
    "                    continue\n",
    "\n",
    "            method = file.split(\"/\")[-1]\n",
    "\n",
    "            old_weight_str = method.split(\"weights_\")[-1].split(\"_\")[0]\n",
    "            a, b = parse_weights(old_weight_str)\n",
    "            if a + b != 10:\n",
    "                continue \n",
    "\n",
    "            if not include_extrema:\n",
    "                if a == 0 or b == 0:\n",
    "                    continue\n",
    "\n",
    "            break_steps = int(file.split(f\"resume_{new_weight_str}_\")[-1].split(\"_\")[0])\n",
    "\n",
    "\n",
    "            print(a, b)\n",
    "\n",
    "            if \"remaining\" in file:\n",
    "                remaining_steps = int(file.split(f\"remaining_\")[-1].split(\"_\")[0])\n",
    "            else:\n",
    "                remaining_steps = 100\n",
    "\n",
    "            runs = os.listdir(file)\n",
    "            for run in runs:\n",
    "\n",
    "                if \"test_\" in run:\n",
    "                    continue\n",
    "\n",
    "                if any([exclude_file in run for exclude_file in files_to_exclude]):\n",
    "                    continue \n",
    "\n",
    "                seed = int(run.split(\"seed_\")[-1].split(\"_\")[0])\n",
    "                checkpoint = int(run.split(\"-\")[-1].split(\".\")[0])\n",
    "\n",
    "\n",
    "                path = os.path.join(file, run)\n",
    "\n",
    "                df = pd.read_pickle(path)\n",
    "\n",
    "                df = df.rename(columns={\"task_idx\": \"skill\", \"task_loss\": \"loss\"})\n",
    "\n",
    "                df[\"method\"] = method\n",
    "                df[\"seed\"] = seed\n",
    "                df[\"checkpoint\"] = checkpoint\n",
    "                df[\"break_steps\"] = break_steps\n",
    "                df[\"remaining_steps\"] = remaining_steps\n",
    "                df[\"new_p1\"] = a\n",
    "                df[\"new_p2\"] = b\n",
    "                df[\"p1\"] = new_a \n",
    "                df[\"p2\"] = new_b\n",
    "\n",
    "\n",
    "                df.set_index(\"checkpoint\", inplace=True)\n",
    "\n",
    "\n",
    "                df_all = pd.concat([df_all, df])\n",
    "\n",
    "\n",
    "    df_all = df_all.sort_values(by=[\"checkpoint\", \"p1\", \"new_p1\", \"seed\"])\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_law(x, param):\n",
    "    k, b = param\n",
    "    # y = c + exp(kx+b)\n",
    "    return k*x + b\n",
    "\n",
    "def param_generator():\n",
    "    for k in np.linspace(-2.4, -1.6, 11):\n",
    "        for b in np.linspace(-1.0, -0.1, 11):\n",
    "            yield [k, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_individual_xy(df_break, df_resume, skill, skills, break_steps, p1):\n",
    "    x = []\n",
    "    y = []\n",
    "    df_break_subset = df_break[(df_break.break_steps == break_steps) & (df_break.p1_normalized == p1) & (df_break.skill == skill)]\n",
    "    df_break_subset = df_break_subset.loc[df_break_subset.index.max()]\n",
    "\n",
    "    df_resume_subset = df_resume[(df_resume.break_steps == break_steps) & (df_resume.p1_normalized == p1) & (df_resume.skill == skill)]\n",
    "    df_resume_subset = df_resume_subset.loc[df_resume_subset.index.max()]\n",
    "    \n",
    "    if skill == skills[0]:\n",
    "        p_col = 'new_p1_normalized'\n",
    "    else:\n",
    "        p_col = 'new_p2_normalized'\n",
    "        \n",
    "    x = df_resume_subset[p_col].values\n",
    "    y = df_resume_subset['loss'].values\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arxiv, Stackexchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [\"../output/09012024/\", \"../output/09022024/\"] # REPLACE WITH YOUR RUN OUTPUT DIRECTORIES\n",
    "task_name = \"slimpj\"\n",
    "slice_list = ['arxiv', 'stackexchange']\n",
    "\n",
    "\n",
    "df_break = load_break_losses(dirs, slice_list, task_name)\n",
    "df_break['p1_normalized'] = df_break.apply(lambda x: x['p1']/(x['p1'] + x['p2']), axis=1)\n",
    "df_break['p2_normalized'] = df_break.apply(lambda x: x['p2']/(x['p1'] + x['p2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = sorted(df_break.skill.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume = load_resume_losses(dirs, skills, task_name, include_extrema=False)\n",
    "df_resume['p1_normalized'] = df_resume.apply(lambda x: x['p1']/(x['p1'] + x['p2']), axis=1)\n",
    "df_resume['p2_normalized'] = df_resume.apply(lambda x: x['p2']/(x['p1'] + x['p2']), axis=1)\n",
    "df_resume['new_p1_normalized'] = df_resume.apply(lambda x: x['new_p1']/(x['new_p1'] + x['new_p2']), axis=1)\n",
    "df_resume['new_p2_normalized'] = df_resume.apply(lambda x: x['new_p2']/(x['new_p1'] + x['new_p2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "break_steps = sorted(df_resume.break_steps.unique())\n",
    "probs = sorted(df_break.p1_normalized.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {skill : {bs: {p: {} for p in probs} for bs in break_steps} for skill in skills }\n",
    "\n",
    "\n",
    "x_per_skill = {skill : {bs: {p: {} for p in probs} for bs in break_steps} for skill in skills }\n",
    "y_per_skill = {skill : {bs: {p: {} for p in probs} for bs in break_steps} for skill in skills }\n",
    "\n",
    "\n",
    "for skill in skills:\n",
    "    for bs in break_steps:\n",
    "        for p1 in probs:\n",
    "            x, y = make_individual_xy(df_break, df_resume, skill, skills, bs, p1)\n",
    "            \n",
    "            x_per_skill[skill][bs][p1] = x\n",
    "            y_per_skill[skill][bs][p1] = y\n",
    "                        \n",
    "            law = ScalingLaw(inter_law)\n",
    "            p = law.fit(x, y, param_generator(), max_step=100, delta=0.02)\n",
    "            params[skill][bs][p1] = p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"./law_results/arxiv_stackexchange/params_dynamic.pkl\", \"wb\") as f:\n",
    "    pickle.dump(params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"DeJavu Serif\"\n",
    "\n",
    "\n",
    "c = np.array([\n",
    "    [31, 119, 180], \n",
    "    [253, 191, 96],   # Golden yellow\n",
    "    [255, 137, 17],   # Orange\n",
    "    [214, 39, 40],    # Red\n",
    "    [255, 152, 150],  # Light pink\n",
    "    [227, 119, 194],  # Pink\n",
    "    [148, 103, 189],  # Purple  # Blue\n",
    "    [44, 160, 44],    # Green\n",
    "    [140, 86, 75],    # Brown\n",
    "    [127, 127, 127],   # Gray,\n",
    "    [23, 190, 207] # Teal\n",
    "]) / 255.0\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 4))\n",
    "fig.subplots_adjust(bottom=0.2, top=1)\n",
    "\n",
    "xs = np.linspace(0, 1, 11)\n",
    "test_break_steps = 2000\n",
    "test_probs = [0.7, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8, 0.9]\n",
    "\n",
    "mses = []\n",
    "r2s = []\n",
    "\n",
    "for k, skill in enumerate(skills):\n",
    "    ax = axes[k]\n",
    "\n",
    "    for j, p1 in enumerate(test_probs):\n",
    "\n",
    "        print(f\"Skill = {skill}\")\n",
    "\n",
    "        x = x_per_skill[skill][test_break_steps][p1]\n",
    "        y = y_per_skill[skill][test_break_steps][p1]\n",
    "\n",
    "        p = params[skill][test_break_steps][p1]\n",
    "\n",
    "        if j != 0:\n",
    "            ax.scatter(x, y, color=c[j], s=300, alpha=0.2)\n",
    "        else:\n",
    "            ax.scatter(x, y, color=c[j], s=300)\n",
    "        plot_preds = inter_law(torch.tensor(xs), torch.tensor(p))\n",
    "\n",
    "        if j != 0:\n",
    "            ax.plot(xs, plot_preds, color=c[j], lw=3, alpha=0.2)\n",
    "        else:\n",
    "            ax.plot(xs, plot_preds, color=c[j], lw=3)\n",
    "\n",
    "        eval_preds = inter_law(torch.tensor(x), torch.tensor(p))\n",
    "        mse = torch.nn.functional.mse_loss(eval_preds, torch.tensor(y)).item()\n",
    "        r2 = r2_score(eval_preds, torch.tensor(y))\n",
    "        print(f\"MSE for {skill}: {mse}\")\n",
    "        print(f\"R2 score for {skill}: {r2}\\n\")\n",
    "\n",
    "        mses.append(mse)\n",
    "        r2s.append(r2)\n",
    "        \n",
    "        #ax.set_yscale(\"log\")\n",
    "        #ax.legend()\n",
    "        ax.set_xlabel(f\"Proportion of {skill}\", fontsize=18)\n",
    "        ax.set_ylabel(f\"Next-step Loss on {skill}\", fontsize=14)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "\n",
    "        ax.grid()\n",
    "legend_elements = []\n",
    "\n",
    "\n",
    "fig.subplots_adjust(bottom=0.1, top=0.85)  \n",
    "fig.suptitle('Linear dynamic mixing law on Arxiv/StackExchange', fontsize=20)\n",
    "\n",
    "#plt.savefig('../figs/arxiv_stackexchange_dynamic.pdf', bbox_inches=\"tight\")\n",
    "\n",
    "mses = np.array(mses)\n",
    "r2s = np.array(r2s)\n",
    "print(f\"MSE: {mses.mean()}, {mses.std()}\")\n",
    "print(f\"R2: {r2s.mean()}, {r2s.std()}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mayeeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
