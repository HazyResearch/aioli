{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "from law import ScalingLaw, MultiObjScalingLaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_weights(weight_str):\n",
    "    if len(weight_str) == 2:\n",
    "        a = int(weight_str[0])\n",
    "        b = int(weight_str[1])\n",
    "    elif len(weight_str) == 3:\n",
    "        if weight_str[0] == \"0\":\n",
    "            a = 0\n",
    "            b = int(weight_str[1:])\n",
    "        elif weight_str[-1] == \"0\":\n",
    "            b = 0 \n",
    "            a = int(weight_str[:2])\n",
    "    else:\n",
    "        idx = [i for i, ltr in enumerate(weight_str) if ltr == \".\"][1] - 1\n",
    "        a = float(weight_str[:idx])\n",
    "        b = float(weight_str[idx:])\n",
    "    return a, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_break_losses(dirs, slice_list, task_name):\n",
    "    files_to_exclude = ['trace', 'val_inputs', 'labels', 'proportions', 'emb', 'rouge', 'generations', 'gradient', 'acc', 'matrix', 'matrices']\n",
    "    df_all = pd.DataFrame() \n",
    "\n",
    "    for dir in dirs:\n",
    "        files = os.listdir(dir)\n",
    "        files = [os.path.join(dir, f) for f in files] \n",
    "        files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "\n",
    "        for file in files:\n",
    "            if \".log\" in file or task_name not in file:\n",
    "                continue \n",
    "\n",
    "            if any(skill not in file for skill in slice_list):\n",
    "                continue \n",
    "\n",
    "            if \"break\" not in file:\n",
    "                continue\n",
    "\n",
    "            break_steps = int(file.split(\"break_\")[-1].split(\"_\")[0])\n",
    "           \n",
    "            method = file.split(\"/\")[-1]\n",
    "\n",
    "            if \"doremi\" not in file:\n",
    "                continue \n",
    "\n",
    "            runs = os.listdir(file)\n",
    "            for run in runs:\n",
    "\n",
    "                if \"test_\" in run:\n",
    "                    continue\n",
    "\n",
    "                if any([exclude_file in run for exclude_file in files_to_exclude]):\n",
    "                    continue \n",
    "\n",
    "                seed = int(run.split(\"seed_\")[-1].split(\"_\")[0])\n",
    "                checkpoint = int(run.split(\"-\")[-1].split(\".\")[0])\n",
    "\n",
    "                if len(df_all) != 0 and len(df_all.loc[(df_all.method==method) & (df_all.seed == seed) & (df_all.index==checkpoint)]) != 0:\n",
    "                    continue \n",
    "\n",
    "                path = os.path.join(file, run)\n",
    "\n",
    "                df = pd.read_pickle(path)\n",
    "\n",
    "                df = df.rename(columns={\"task_idx\": \"skill\", \"task_loss\": \"loss\"})\n",
    "\n",
    "                df[\"method\"] = method\n",
    "                df[\"seed\"] = seed\n",
    "                df[\"checkpoint\"] = checkpoint\n",
    "                df[\"break_steps\"] = break_steps\n",
    "\n",
    "                df.set_index(\"checkpoint\", inplace=True)\n",
    "\n",
    "\n",
    "                df_all = pd.concat([df_all, df])\n",
    "\n",
    "\n",
    "    df_all = df_all.sort_values(by=[\"checkpoint\", \"seed\"])\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resume_losses(dirs, slice_list, task_name):\n",
    "    files_to_exclude = ['trace', 'val_inputs', 'labels', 'proportions', 'emb', 'rouge', 'generations', 'gradient', 'acc', 'matrix', 'matrices']\n",
    "    df_all = pd.DataFrame() \n",
    "\n",
    "    for dir in dirs:\n",
    "        files = os.listdir(dir)\n",
    "        files = [os.path.join(dir, f) for f in files] # add path to each file\n",
    "        files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "\n",
    "        for file in files:\n",
    "            if \".log\" in file or task_name not in file:\n",
    "                continue \n",
    "\n",
    "            if any(skill not in file for skill in slice_list):\n",
    "                continue \n",
    "\n",
    "            if \"resume\" not in file:\n",
    "                continue\n",
    "\n",
    "            if \"doremi\" not in file:\n",
    "                continue \n",
    "\n",
    "\n",
    "            method = file.split(\"/\")[-1]\n",
    "\n",
    "            weight_str = method.split(\"weights_\")[-1].split(\"_\")[0]\n",
    "            a, b = parse_weights(weight_str)\n",
    "\n",
    "            if a + b != 10:\n",
    "                continue \n",
    "\n",
    "            if a == 0 or b == 0:\n",
    "                continue\n",
    "\n",
    "            print(a, b)\n",
    "            break_steps = int(file.split(f\"resume_doremi_\")[-1].split(\"_\")[0])\n",
    "\n",
    "            runs = os.listdir(file)\n",
    "            for run in runs:\n",
    "\n",
    "                if \"test_\" in run:\n",
    "                    continue\n",
    "\n",
    "                if any([exclude_file in run for exclude_file in files_to_exclude]):\n",
    "                    continue \n",
    "\n",
    "                seed = int(run.split(\"seed_\")[-1].split(\"_\")[0])\n",
    "                checkpoint = int(run.split(\"-\")[-1].split(\".\")[0])\n",
    "                if len(df_all) != 0 and len(df_all.loc[(df_all.method==method) & (df_all.seed == seed) & (df_all.index==checkpoint)]) != 0:\n",
    "                    continue \n",
    "\n",
    "\n",
    "                path = os.path.join(file, run)\n",
    "\n",
    "                df = pd.read_pickle(path)\n",
    "\n",
    "                df = df.rename(columns={\"task_idx\": \"skill\", \"task_loss\": \"loss\"})\n",
    "\n",
    "                df[\"method\"] = method\n",
    "                df[\"seed\"] = seed\n",
    "                df[\"checkpoint\"] = checkpoint\n",
    "                df[\"break_steps\"] = break_steps\n",
    "                df[\"new_p1\"] = a \n",
    "                df[\"new_p2\"] = b \n",
    "\n",
    "                df.set_index(\"checkpoint\", inplace=True)\n",
    "\n",
    "\n",
    "                df_all = pd.concat([df_all, df])\n",
    "\n",
    "\n",
    "    df_all = df_all.sort_values(by=[\"checkpoint\", \"new_p1\", \"seed\"])\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doremi_matrices(dirs, slice_list, break_steps):\n",
    "    matrices = {}\n",
    "\n",
    "    slice_str = \"_\".join(slice_list)\n",
    "    for dir in dirs:\n",
    "        files = os.listdir(dir)\n",
    "        files = [os.path.join(dir, f) for f in files]\n",
    "\n",
    "        for file in files:\n",
    "            \n",
    "            if \"break\" in file and f\"stratified_{slice_str}_doremi\" in file:\n",
    "\n",
    "                s = int(file.split(\"_break_\")[-1].split(\"_\")[0])\n",
    "                if s != break_steps:\n",
    "                    continue\n",
    "                runs = os.listdir(file)\n",
    "                for run in runs:\n",
    "                    if \"avg\" not in run:\n",
    "                        continue\n",
    "                    if \"drm_matrices.npy\" not in run:\n",
    "                        continue \n",
    "                    path = os.path.join(file, run)\n",
    "                    print(path)\n",
    "                    seed = int(run.split(\"seed_\")[-1].split(\"_\")[0])\n",
    "\n",
    "                    \n",
    "\n",
    "                    A = np.load(path)\n",
    "                    matrices[seed] = A.mean(axis=0)\n",
    "\n",
    "    return matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_law(x, param):\n",
    "    k, b = param\n",
    "    return k*x + b\n",
    "\n",
    "def param_generator():\n",
    "    for k in np.linspace(-2.4, -1.6, 11):\n",
    "        for b in np.linspace(-1.0, -0.1, 11):\n",
    "            yield [k, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_individual_xy(df_break, df_resume, skill, skills, break_steps, seed):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    df_break_subset = df_break[(df_break.break_steps == break_steps) & (df_break.seed == seed) & (df_break.skill == skill)]\n",
    "    df_break_subset = df_break_subset.loc[df_break_subset.index.max()]\n",
    "\n",
    "    df_resume_subset = df_resume[(df_resume.break_steps == break_steps) & (df_resume.seed == seed) & (df_resume.skill == skill)]\n",
    "    df_resume_subset = df_resume_subset.loc[df_resume_subset.index.max()]\n",
    "    \n",
    "    if skill == skills[0]:\n",
    "        p_col = 'new_p1_normalized'\n",
    "    else:\n",
    "        p_col = 'new_p2_normalized'\n",
    "        \n",
    "    x = df_resume_subset[p_col].values\n",
    "    y = df_resume_subset['loss'].values\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_xy_joint(df_break, df_resume, matrix, break_steps, seed):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    df_break_subset = df_break[(df_break.break_steps == break_steps) & (df_break.seed == seed)]\n",
    "    df_break_subset = df_break_subset.loc[df_break_subset.index.max()]\n",
    "\n",
    "    df_resume_subset = df_resume[(df_resume.break_steps == break_steps) & (df_resume.seed == seed)]\n",
    "    df_resume_subset = df_resume_subset.loc[df_resume_subset.index.max()]\n",
    "\n",
    "    x = df_resume_subset[['new_p1_normalized', 'new_p2_normalized']].drop_duplicates(keep='first').values.T\n",
    "    x = matrix.dot(x)\n",
    "    y = df_resume_subset['loss'].values.reshape(-1, 2)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def law_1(x, param):\n",
    "    b = param[0]\n",
    "    c1 = param[1]\n",
    "    return c1 + b*x[0]\n",
    "\n",
    "def law_2(x, param):\n",
    "    b = param[0]\n",
    "    c2 = param[2]\n",
    "    return c2 + b*x[1]\n",
    "\n",
    "\n",
    "def param_generator_joint():\n",
    "    for b in np.linspace(-10, 0, 11):\n",
    "        for c1 in np.linspace(0.0, 1.0, 11):\n",
    "            for c2 in np.linspace(0.0, 1.0, 11):\n",
    "                yield [b, c1, c2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arxiv, Stackexchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [\"../output/09252024/\"] # REPLACE WITH YOUR RUN OUTPUT DIRECTORIES\n",
    "task_name = \"slimpj\"\n",
    "slice_list = ['arxiv', 'stackexchange']\n",
    "df_break = load_break_losses(dirs, slice_list, task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = sorted(df_break.skill.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume = load_resume_losses(dirs, skills, task_name)\n",
    "df_resume['new_p1_normalized'] = df_resume.apply(lambda x: x['new_p1']/(x['new_p1'] + x['new_p2']), axis=1)\n",
    "df_resume['new_p2_normalized'] = df_resume.apply(lambda x: x['new_p2']/(x['new_p1'] + x['new_p2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "break_steps = sorted(df_resume.break_steps.unique())\n",
    "seeds = sorted(df_break.seed.unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get $A^{t \\star}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {skill : {bs: {seed: {} for seed in seeds} for bs in break_steps} for skill in skills }\n",
    "x_per_skill = {skill : {bs: {seed: {} for seed in seeds} for bs in break_steps} for skill in skills }\n",
    "y_per_skill = {skill : {bs: {seed: {} for seed in seeds} for bs in break_steps} for skill in skills }\n",
    "\n",
    "for skill in skills:\n",
    "    for bs in [500]:\n",
    "        for seed in seeds:\n",
    "            print(skill, bs, seed)\n",
    "            x, y = make_individual_xy(df_break, df_resume, skill, skills, bs, seed)\n",
    "        \n",
    "            x_per_skill[skill][bs][seed] = x\n",
    "            y_per_skill[skill][bs][seed] = y\n",
    "                        \n",
    "            law = ScalingLaw(inter_law)\n",
    "            p = law.fit(x, y, param_generator(), max_step=100, delta=0.02)\n",
    "            params[skill][bs][seed] = [p[0], p[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"./law_results/arxiv_stackexchange/params_doremi_trajectory_opt_500.pkl\", \"wb\") as f:\n",
    "    pickle.dump(params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get $\\tilde{A}^t = b^t A^t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doremi_matrices = load_doremi_matrices(dirs, slice_list, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_doremi = {bs: {seed: {} for seed in seeds} for bs in break_steps}\n",
    "\n",
    "x_per_skill_doremi = {bs: {seed: {} for seed in seeds} for bs in break_steps}\n",
    "y_per_skill_doremi = {bs: {seed: {} for seed in seeds} for bs in break_steps}\n",
    "\n",
    "for bs in [500]:\n",
    "    for seed in seeds:\n",
    "        print(bs, seed)\n",
    "        x, ys = make_xy_joint(df_break, df_resume, doremi_matrices[seed], bs, seed)\n",
    "        \n",
    "        x_per_skill_doremi[bs][seed] = x\n",
    "        y_per_skill_doremi[bs][seed] = ys\n",
    "                    \n",
    "        law = MultiObjScalingLaw([law_1, law_2])\n",
    "        p = law.fit(x, ys.T, param_generator_joint(), max_step=100, delta=0.02)\n",
    "        params_doremi[bs][seed] = p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"./law_results/arxiv_stackexchange/params_doremi_trajectory_doremi_matrix_500.pkl\", \"wb\") as f:\n",
    "    pickle.dump(params_doremi, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mayeeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
