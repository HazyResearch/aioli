{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "from law import ScalingLaw, MultiObjScalingLaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_break_losses(dirs, slice_list, task_name):\n",
    "    files_to_exclude = ['trace', 'val_inputs', 'labels', 'proportions', 'emb', 'rouge', 'generations', 'gradient', 'acc', 'matrices']\n",
    "    df_all = pd.DataFrame() \n",
    "\n",
    "    for dir in dirs:\n",
    "        files = os.listdir(dir)\n",
    "        files = [os.path.join(dir, f) for f in files] \n",
    "        files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "\n",
    "        for file in files:\n",
    "            if \".log\" in file or task_name not in file:\n",
    "                continue \n",
    "\n",
    "            if any(skill not in file for skill in slice_list):\n",
    "                continue \n",
    "\n",
    "            if \"break\" not in file:\n",
    "                continue\n",
    "\n",
    "            break_steps = int(file.split(\"break_\")[-1].split(\"_\")[0])\n",
    "           \n",
    "            method = file.split(\"/\")[-1]\n",
    "\n",
    "            if \"skillit\" not in file:\n",
    "                continue \n",
    "\n",
    "            runs = os.listdir(file)\n",
    "            for run in runs:\n",
    "\n",
    "                if \"test_\" in run:\n",
    "                    continue\n",
    "\n",
    "                if any([exclude_file in run for exclude_file in files_to_exclude]):\n",
    "                    continue \n",
    "\n",
    "                seed = int(run.split(\"seed_\")[-1].split(\"_\")[0])\n",
    "                checkpoint = int(run.split(\"-\")[-1].split(\".\")[0])\n",
    "\n",
    "\n",
    "                path = os.path.join(file, run)\n",
    "\n",
    "                df = pd.read_pickle(path)\n",
    "\n",
    "                df = df.rename(columns={\"task_idx\": \"skill\", \"task_loss\": \"loss\"})\n",
    "\n",
    "                df[\"method\"] = method\n",
    "                df[\"seed\"] = seed\n",
    "                df[\"checkpoint\"] = checkpoint\n",
    "                df[\"break_steps\"] = break_steps\n",
    "\n",
    "\n",
    "                df.set_index(\"checkpoint\", inplace=True)\n",
    "\n",
    "\n",
    "                df_all = pd.concat([df_all, df])\n",
    "\n",
    "\n",
    "    df_all = df_all.sort_values(by=[\"checkpoint\", \"seed\"])\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_resume_losses(dirs, slice_list, task_name):\n",
    "    files_to_exclude = ['trace', 'val_inputs', 'labels', 'proportions', 'emb', 'rouge', 'generations', 'gradient', 'acc', 'matrices']\n",
    "    df_all = pd.DataFrame() \n",
    "\n",
    "    for dir in dirs:\n",
    "        files = os.listdir(dir)\n",
    "        files = [os.path.join(dir, f) for f in files] \n",
    "        files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "\n",
    "        for file in files:\n",
    "            if \".log\" in file or task_name not in file:\n",
    "                continue \n",
    "\n",
    "            if any(skill not in file for skill in slice_list):\n",
    "                continue \n",
    "\n",
    "            if \"resume\" not in file:\n",
    "                continue\n",
    "\n",
    "            if \"skillit\" not in file:\n",
    "                continue \n",
    "\n",
    "\n",
    "            method = file.split(\"/\")[-1]\n",
    "\n",
    "            new_weight_str = method.split(\"weights_\")[-1].split(\"_\")[0]\n",
    "            new_a, new_b, new_c = [float(f\"0.{weight}\") for weight in new_weight_str.split(\"0.\")[1:]]\n",
    "\n",
    "\n",
    "            break_steps = int(file.split(f\"resume_skillit_\")[-1].split(\"_\")[0])\n",
    "\n",
    "\n",
    "            runs = os.listdir(file)\n",
    "            for run in runs:\n",
    "\n",
    "                if \"test_\" in run:\n",
    "                    continue\n",
    "\n",
    "                if any([exclude_file in run for exclude_file in files_to_exclude]):\n",
    "                    continue \n",
    "\n",
    "                seed = int(run.split(\"seed_\")[-1].split(\"_\")[0])\n",
    "                checkpoint = int(run.split(\"-\")[-1].split(\".\")[0])\n",
    "\n",
    "\n",
    "                path = os.path.join(file, run)\n",
    "\n",
    "                df = pd.read_pickle(path)\n",
    "\n",
    "                df = df.rename(columns={\"task_idx\": \"skill\", \"task_loss\": \"loss\"})\n",
    "\n",
    "                df[\"method\"] = method\n",
    "                df[\"seed\"] = seed\n",
    "                df[\"checkpoint\"] = checkpoint\n",
    "                df[\"break_steps\"] = break_steps\n",
    "                df[\"new_p1\"] = new_a \n",
    "                df[\"new_p2\"] = new_b \n",
    "                df[\"new_p3\"] = new_c \n",
    "\n",
    "                df.set_index(\"checkpoint\", inplace=True)\n",
    "\n",
    "\n",
    "                df_all = pd.concat([df_all, df])\n",
    "\n",
    "\n",
    "    df_all = df_all.sort_values(by=[\"checkpoint\", \"new_p1\", \"seed\"])\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_skillit_matrices(slice_list, seed, task_name):\n",
    "    A = np.load(f\"../skillit_graphs/{task_name}_{'_'.join(slice_list)}_normalized_seed_{seed}.npy\")\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r_squared(actuals, predictions):\n",
    "    actuals, predictions = actuals.numpy(), predictions.numpy()\n",
    "    # Calculate the total sum of squares\n",
    "    total_sum_of_squares = np.sum((actuals - np.mean(actuals)) ** 2)\n",
    "    # Calculate the residual sum of squares\n",
    "    residual_sum_of_squares = np.sum((actuals - predictions) ** 2)\n",
    "    # Calculate R-squared\n",
    "    r_squared = 1 - (residual_sum_of_squares / total_sum_of_squares)\n",
    "    return r_squared\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixing_law_full(x, param):\n",
    "    # one set of params per skill\n",
    "    #print(param)\n",
    "\n",
    "    result = torch.matmul(x, param)\n",
    "    return result\n",
    "\n",
    "def init_params_law_full(idx, num_domains=3):\n",
    "    #for c_i in np.linspace(0.5, 5, 10):\n",
    "    for _ in range(30):\n",
    "        ts = [-np.random.rand() if i == idx else np.random.rand() * 0.1 for i in range(num_domains)]\n",
    "        yield ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_individual_xy_full(df_break, df_resume, skill, break_steps, seed):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    df_break_subset = df_break[(df_break.break_steps == break_steps) & (df_break.seed == seed) & (df_break.skill == skill)]\n",
    "    df_break_subset = df_break_subset.loc[df_break_subset.index.max()]\n",
    "\n",
    "    df_resume_subset = df_resume[(df_resume.break_steps == break_steps) & (df_resume.seed == seed) & (df_resume.skill == skill)]\n",
    "    df_resume_subset = df_resume_subset.loc[df_resume_subset.index.max()]\n",
    "    \n",
    "        \n",
    "    loss_0 = df_break_subset.loss\n",
    "    x = df_resume_subset[['new_p1', 'new_p2', 'new_p3']].values\n",
    "    y = df_resume_subset['loss'].values - loss_0\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_xy_joint(df_break, df_resume, A, break_steps, seed):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    df_break_subset = df_break[(df_break.break_steps == break_steps) & (df_break.seed == seed)]\n",
    "    df_break_subset = df_break_subset.loc[df_break_subset.index.max()]\n",
    "\n",
    "    df_resume_subset = df_resume[(df_resume.break_steps == break_steps) & (df_resume.seed == seed)]\n",
    "    df_resume_subset = df_resume_subset.loc[df_resume_subset.index.max()]\n",
    "\n",
    "    x = df_resume_subset[['new_p1_normalized', 'new_p2_normalized', 'new_p3_normalized']].drop_duplicates(keep='first').values\n",
    "    x = A.T.dot(x.T) # we have to transpose A because all of the skill-it runs have transposed the A matrix (we kept the original skill-it code)\n",
    "\n",
    "    l0 = np.tile(df_break_subset['loss'].values, reps=(x.shape[1], 1)).T\n",
    "\n",
    "    x = np.multiply(l0, x)\n",
    "\n",
    "    y = df_resume_subset['loss'].values.reshape(-1, 3) - df_break_subset.loss.values\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def law_1(x, param):\n",
    "    b = param[0]\n",
    "    return b*x[0]\n",
    "\n",
    "def law_2(x, param):\n",
    "    b = param[0]\n",
    "    return b*x[1]\n",
    "\n",
    "def law_3(x, param):\n",
    "    b = param[0]\n",
    "    return b*x[2]\n",
    "\n",
    "\n",
    "def param_generator_joint():\n",
    "    for b in np.linspace(-10, 0, 5):\n",
    "        yield [b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arxiv Book Stackexchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [\"../output/09272024/\", \"../output/09262024/\"] # REPLACE WITH YOUR RUN OUTPUT DIRECTORIES\n",
    "task_name = \"slimpj\"\n",
    "slice_list = ['arxiv', 'book', 'stackexchange']\n",
    "df_break = load_break_losses(dirs, slice_list, task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = sorted(df_break.skill.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resume = load_resume_losses(dirs, skills, task_name)\n",
    "df_resume['new_p1_normalized'] = df_resume.apply(lambda x: x['new_p1']/(x['new_p1'] + x['new_p2'] + x['new_p3']), axis=1)\n",
    "df_resume['new_p2_normalized'] = df_resume.apply(lambda x: x['new_p2']/(x['new_p1'] + x['new_p2'] + x['new_p3']), axis=1)\n",
    "df_resume['new_p3_normalized'] = df_resume.apply(lambda x: x['new_p3']/(x['new_p1'] + x['new_p2'] + x['new_p3']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "break_steps = sorted(df_resume.break_steps.unique())\n",
    "seeds = sorted(df_break.seed.unique())\n",
    "\n",
    "break_steps = [1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get $A^{t \\star}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv 1000 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 11.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss: 0.0001222293358296156\n",
      "optimal_param: tensor([-0.2545,  0.0382, -0.0666])\n",
      "MSE: 0.0002700971672311425, R2: 0.9429931755700922\n",
      "arxiv 1000 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/19831/ipykernel_1552607/1531310371.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  r2_train = calculate_r_squared(torch.tensor(y), torch.tensor(prediction_train))\n",
      "100%|██████████| 30/30 [00:01<00:00, 18.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss: 5.9037811297457665e-05\n",
      "optimal_param: tensor([-0.3140,  0.0789, -0.0502])\n",
      "MSE: 0.00011809913848992437, R2: 0.9874185074514449\n",
      "arxiv 1000 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/19831/ipykernel_1552607/1531310371.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  r2_train = calculate_r_squared(torch.tensor(y), torch.tensor(prediction_train))\n",
      "100%|██████████| 30/30 [00:01<00:00, 18.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss: 9.362303535453975e-05\n",
      "optimal_param: tensor([-0.2829,  0.0288, -0.0522])\n",
      "MSE: 0.0002106520114466548, R2: 0.9658413612803263\n",
      "arxiv 1000 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/19831/ipykernel_1552607/1531310371.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  r2_train = calculate_r_squared(torch.tensor(y), torch.tensor(prediction_train))\n",
      "100%|██████████| 30/30 [00:01<00:00, 18.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss: 0.00011416756024118513\n",
      "optimal_param: tensor([-0.4235, -0.0128, -0.1170])\n",
      "MSE: 0.0002447843726258725, R2: 0.9738225407526638\n",
      "arxiv 1000 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/19831/ipykernel_1552607/1531310371.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  r2_train = calculate_r_squared(torch.tensor(y), torch.tensor(prediction_train))\n",
      "100%|██████████| 30/30 [00:01<00:00, 18.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss: 0.00019273950601927936\n",
      "optimal_param: tensor([-0.3924,  0.0383, -0.1098])\n",
      "MSE: 0.0005644520279020071, R2: 0.9448499495047843\n",
      "book 1000 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/19831/ipykernel_1552607/1531310371.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  r2_train = calculate_r_squared(torch.tensor(y), torch.tensor(prediction_train))\n",
      "100%|██████████| 30/30 [00:01<00:00, 17.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss: 0.0002081194834318012\n",
      "optimal_param: tensor([ 0.0353, -0.2970, -0.0181])\n",
      "MSE: 0.000556315528228879, R2: 0.9129804942733457\n",
      "book 1000 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/19831/ipykernel_1552607/1531310371.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  r2_train = calculate_r_squared(torch.tensor(y), torch.tensor(prediction_train))\n",
      "100%|██████████| 30/30 [00:01<00:00, 18.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss: 7.42196207283996e-05\n",
      "optimal_param: tensor([ 0.0223, -0.2738,  0.0433])\n",
      "MSE: 0.0001484392414567992, R2: 0.9801734216132441\n",
      "book 1000 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/19831/ipykernel_1552607/1531310371.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  r2_train = calculate_r_squared(torch.tensor(y), torch.tensor(prediction_train))\n",
      "100%|██████████| 30/30 [00:01<00:00, 17.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss: 9.882343874778599e-05\n",
      "optimal_param: tensor([ 0.0544, -0.2720,  0.0536])\n",
      "MSE: 0.00019994202011730522, R2: 0.9740782138848112\n",
      "book 1000 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/19831/ipykernel_1552607/1531310371.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  r2_train = calculate_r_squared(torch.tensor(y), torch.tensor(prediction_train))\n",
      "100%|██████████| 30/30 [00:01<00:00, 18.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss: 2.3274658815353177e-05\n",
      "optimal_param: tensor([ 0.0292, -0.2874,  0.0097])\n",
      "MSE: 4.6549317630706355e-05, R2: 0.9921175420864888\n",
      "book 1000 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/19831/ipykernel_1552607/1531310371.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  r2_train = calculate_r_squared(torch.tensor(y), torch.tensor(prediction_train))\n",
      "100%|██████████| 30/30 [00:01<00:00, 16.40it/s]\n",
      "/tmp/user/19831/ipykernel_1552607/1531310371.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  r2_train = calculate_r_squared(torch.tensor(y), torch.tensor(prediction_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss: 0.0004254251252859831\n",
      "optimal_param: tensor([ 0.0597, -0.2962,  0.0106])\n",
      "MSE: 0.001375290798023343, R2: 0.8199159035123161\n",
      "stackexchange 1000 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:01<00:00, 17.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss: 0.00018710180302150548\n",
      "optimal_param: tensor([ 0.0516,  0.0782, -0.2213])\n",
      "MSE: 0.0005905625293962657, R2: 0.8868305452480219\n",
      "stackexchange 1000 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/19831/ipykernel_1552607/1531310371.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  r2_train = calculate_r_squared(torch.tensor(y), torch.tensor(prediction_train))\n",
      "100%|██████████| 30/30 [00:02<00:00, 14.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss: 0.0001999284722842276\n",
      "optimal_param: tensor([-0.0014,  0.0873, -0.2832])\n",
      "MSE: 0.0004767045029439032, R2: 0.9401516801345832\n",
      "stackexchange 1000 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/19831/ipykernel_1552607/1531310371.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  r2_train = calculate_r_squared(torch.tensor(y), torch.tensor(prediction_train))\n",
      "100%|██████████| 30/30 [00:01<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss: 0.00015331119357142597\n",
      "optimal_param: tensor([ 0.0062,  0.0779, -0.2767])\n",
      "MSE: 0.00035897427005693316, R2: 0.948159928237459\n",
      "stackexchange 1000 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/19831/ipykernel_1552607/1531310371.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  r2_train = calculate_r_squared(torch.tensor(y), torch.tensor(prediction_train))\n",
      "100%|██████████| 30/30 [00:02<00:00, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss: 0.0001651076163398102\n",
      "optimal_param: tensor([-0.0355,  0.0522, -0.2519])\n",
      "MSE: 0.000381678604753688, R2: 0.9345704259691378\n",
      "stackexchange 1000 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/19831/ipykernel_1552607/1531310371.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  r2_train = calculate_r_squared(torch.tensor(y), torch.tensor(prediction_train))\n",
      "100%|██████████| 30/30 [00:01<00:00, 17.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss: 7.297917181858793e-05\n",
      "optimal_param: tensor([-0.0190,  0.0638, -0.3009])\n",
      "MSE: 0.0001464509405195713, R2: 0.9771432812766424\n",
      "0.00037926616472153303 0.0003150922756394066\n",
      "0.9454031313863575 0.043664815197527615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/19831/ipykernel_1552607/1531310371.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  r2_train = calculate_r_squared(torch.tensor(y), torch.tensor(prediction_train))\n"
     ]
    }
   ],
   "source": [
    "params = {skill : {bs: {seed: {} for seed in seeds} for bs in break_steps} for skill in skills }\n",
    "x_per_skill = {skill : {bs: {seed: {} for seed in seeds} for bs in break_steps} for skill in skills }\n",
    "y_per_skill = {skill : {bs: {seed: {} for seed in seeds} for bs in break_steps} for skill in skills }\n",
    "\n",
    "mses = []\n",
    "r2s = []\n",
    "for i, skill in enumerate(skills):\n",
    "    for bs in break_steps:\n",
    "        for seed in seeds:\n",
    "            print(skill, bs, seed)\n",
    "            x, y = make_individual_xy_full(df_break, df_resume, skill, bs, seed)\n",
    "        \n",
    "            x_per_skill[skill][bs][seed] = x\n",
    "            y_per_skill[skill][bs][seed] = y\n",
    "                        \n",
    "            law = ScalingLaw(mixing_law_full)\n",
    "            p = law.fit(x, y, init_params_law_full(i, num_domains=len(skills)), max_step=100, delta=0.02)\n",
    "            params[skill][bs][seed] = p # param\n",
    "\n",
    "            prediction_train = mixing_law_full(torch.tensor(x, dtype=torch.float), torch.tensor(p, dtype=torch.float))\n",
    "            mse_train = torch.nn.functional.mse_loss(prediction_train, torch.tensor(y, dtype=torch.float)).item()\n",
    "            r2_train = calculate_r_squared(torch.tensor(y), torch.tensor(prediction_train))\n",
    "\n",
    "            mses.append(mse_train)\n",
    "            r2s.append(r2_train)\n",
    "\n",
    "\n",
    "            print(f\"MSE: {mse_train}, R2: {r2_train}\")\n",
    "\n",
    "\n",
    "mses = np.array(mses)\n",
    "r2s = np.array(r2s)\n",
    "\n",
    "print(mses.mean(), mses.std())\n",
    "print(r2s.mean(), r2s.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"./law_results/arxiv_books_stackexchange/params_skillit_trajectory_opt_1000.pkl\", \"wb\") as f:\n",
    "    pickle.dump(params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get $\\tilde{A}^t = b^t A^t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "skillit_matrices = {}\n",
    "for seed in seeds:\n",
    "    skillit_matrices[seed] = load_skillit_matrices(slice_list, seed, task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0\n",
      "workers: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss: 0.02710052113980055\n",
      "optimal_param: tensor([-0.0275])\n",
      "1000 1\n",
      "workers: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss: 0.03443594090640545\n",
      "optimal_param: tensor([-0.0276])\n",
      "1000 2\n",
      "workers: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss: 0.03130367677658796\n",
      "optimal_param: tensor([-0.0283])\n",
      "1000 3\n",
      "workers: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss: 0.03555733431130648\n",
      "optimal_param: tensor([-0.0414])\n",
      "1000 4\n",
      "workers: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min loss: 0.03412032313644886\n",
      "optimal_param: tensor([-0.0439])\n"
     ]
    }
   ],
   "source": [
    "params_skillit = {bs: {seed: {} for seed in seeds} for bs in break_steps}\n",
    "\n",
    "\n",
    "x_per_skill_skillit = {bs: {seed: {} for seed in seeds} for bs in break_steps}\n",
    "y_per_skill_skillit = {bs: {seed: {} for seed in seeds} for bs in break_steps}\n",
    "\n",
    "for bs in break_steps:\n",
    "    for seed in seeds:\n",
    "        print(bs, seed)\n",
    "        x, ys = make_xy_joint(df_break, df_resume, skillit_matrices[seed], bs, seed)\n",
    "        \n",
    "        x_per_skill_skillit[bs][seed] = x\n",
    "        y_per_skill_skillit[bs][seed] = ys\n",
    "                    \n",
    "        law = MultiObjScalingLaw([law_1, law_2, law_3])\n",
    "        p = law.fit(x, ys.T, param_generator_joint(), max_step=100, delta=0.02)\n",
    "        params_skillit[bs][seed] = p # param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"./law_results/arxiv_books_stackexchange/params_skillit_trajectory_skillit_matrix_1000.pkl\", \"wb\") as f:\n",
    "    pickle.dump(params_skillit, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mayeeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
